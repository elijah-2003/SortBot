{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "858d5c81f5fd472dbf81bf179797be45",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## **Movement and Static Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "fb60b1ef22fd455b9f19ef422956d9c6",
    "deepnote_cell_type": "code",
    "execution_context_id": "301fdf73-8afa-40db-a681-9791010114a3",
    "execution_millis": 2689,
    "execution_start": 1732475937207,
    "source_hash": "fecae10a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pydrake.all import (\n",
    "    AddDefaultVisualization,\n",
    "    AddMultibodyPlantSceneGraph,\n",
    "    DiagramBuilder,\n",
    "    LoadModelDirectives,\n",
    "    LoadModelDirectivesFromString,\n",
    "    Parser,\n",
    "    ProcessModelDirectives,\n",
    "    RigidTransform,\n",
    "    RollPitchYaw,\n",
    "    Simulator,\n",
    "    StartMeshcat,\n",
    "    PassThrough, ConstantVectorSource,Adder,\n",
    "    SpatialInertia, UnitInertia, RevoluteJoint,Integrator, Multiplexer\n",
    ")\n",
    "from pydrake.common import temp_directory\n",
    "from pydrake.geometry import StartMeshcat\n",
    "from pydrake.systems.analysis import Simulator\n",
    "from pydrake.visualization import ModelVisualizer\n",
    "\n",
    "from manipulation import running_as_notebook\n",
    "from manipulation.station import LoadScenario, MakeHardwareStation\n",
    "from manipulation.utils import ConfigureParser\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from pydrake.all import (\n",
    "    AddDefaultVisualization,\n",
    "    AddMultibodyPlantSceneGraph,\n",
    "    DiagramBuilder,\n",
    "    LoadModelDirectives,\n",
    "    LoadModelDirectivesFromString,\n",
    "    Parser,\n",
    "    ProcessModelDirectives,\n",
    "    RigidTransform,\n",
    "    RollPitchYaw,\n",
    "    Simulator,\n",
    "    StartMeshcat,\n",
    "    MeshcatVisualizer\n",
    ")\n",
    "from pydrake.common import temp_directory\n",
    "from pydrake.geometry import StartMeshcat\n",
    "from pydrake.systems.analysis import Simulator\n",
    "from pydrake.visualization import ModelVisualizer\n",
    "\n",
    "\n",
    "from manipulation.utils import RenderDiagram\n",
    "from manipulation.meshcat_utils import AddMeshcatTriad\n",
    "from manipulation import running_as_notebook\n",
    "from manipulation.station import LoadScenario, MakeHardwareStation\n",
    "from manipulation.utils import ConfigureParser\n",
    "\n",
    "from pydrake.all import (\n",
    "    DiagramBuilder, Simulator, RigidTransform, RotationMatrix,\n",
    "    InverseKinematics, Solve, MultibodyPlant, Parser, DirectCollocation, MathematicalProgram,\n",
    ")\n",
    "\n",
    "from manipulation.meshcat_utils import AddMeshcatTriad\n",
    "from pydrake.all import RotationMatrix, FixedOffsetFrame, MultibodyPlant, InverseKinematics, Solve, RotationMatrix\n",
    "from pydrake.all import PiecewisePolynomial, TrajectorySource, ConstantVectorSource, Box, Rgba\n",
    "from pydrake.symbolic import Formula\n",
    "from pydrake.all import PiecewisePolynomial, TrajectorySource, ConstantVectorSource, Box, Rgba\n",
    "import pydot\n",
    "from scipy.spatial import KDTree\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from copy import deepcopy\n",
    "from urllib.request import urlretrieve\n",
    "from pydrake.all import (\n",
    "    AddDefaultVisualization,\n",
    "    AddMultibodyPlantSceneGraph,\n",
    "    DiagramBuilder,\n",
    "    LoadModelDirectives,\n",
    "    LoadModelDirectivesFromString,\n",
    "    Parser,\n",
    "    ProcessModelDirectives,\n",
    "    RigidTransform,\n",
    "    RollPitchYaw,\n",
    "    Simulator,\n",
    "    StartMeshcat,\n",
    "    ProcessModelDirectives, \n",
    "    LoadModelDirectives,\n",
    "    RgbdSensor, \n",
    "    RigidTransform, \n",
    "    RollPitchYaw,\n",
    "    ColorRenderCamera,\n",
    "    DepthRenderCamera,\n",
    "    MeshcatVisualizer,\n",
    "    MeshcatVisualizerParams\n",
    ")\n",
    "from pydrake.common import temp_directory\n",
    "from pydrake.geometry import StartMeshcat\n",
    "from pydrake.systems.analysis import Simulator\n",
    "from pydrake.visualization import ModelVisualizer\n",
    "from manipulation import running_as_notebook\n",
    "from manipulation.station import AddPointClouds, LoadScenario, MakeHardwareStation, Scenario\n",
    "from manipulation.utils import ConfigureParser\n",
    "from manipulation.scenarios import AddMultibodyTriad\n",
    "from manipulation.meshcat_utils import AddMeshcatTriad\n",
    "from pydrake.all import RotationMatrix, FixedOffsetFrame, MultibodyPlant, InverseKinematics, Solve, RotationMatrix\n",
    "from pydrake.all import PiecewisePolynomial, TrajectorySource, ConstantVectorSource, Box, Rgba\n",
    "from pydrake.symbolic import Formula\n",
    "from pydrake.all import PiecewisePolynomial, TrajectorySource, ConstantVectorSource, Box, Rgba\n",
    "import pydot\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.stats import mode\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as Tf\n",
    "from pydrake.all import (\n",
    "    BaseField,\n",
    "    Concatenate,\n",
    "    Fields,\n",
    "    MeshcatVisualizer,\n",
    "    MeshcatVisualizerParams,\n",
    "    PointCloud,\n",
    "    StartMeshcat,\n",
    ")\n",
    "from pydrake.multibody.parsing import Parser\n",
    "from pydrake.multibody.plant import AddMultibodyPlantSceneGraph\n",
    "from pydrake.systems.framework import DiagramBuilder\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from manipulation import running_as_notebook\n",
    "from manipulation.clutter import GenerateAntipodalGraspCandidate\n",
    "from manipulation.scenarios import AddRgbdSensors\n",
    "from manipulation.utils import ConfigureParser, FindDataResource\n",
    "from manipulation.meshcat_utils import AddMeshcatTriad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "3a221dbf0e1448b9ac9e53ba4271455c",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "20cbcee3-6169-4b2e-ad2f-a4649e3ecafc",
    "execution_millis": 9,
    "execution_start": 1732475939941,
    "is_code_hidden": false,
    "source_hash": "450eddfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:drake:Meshcat listening for connections at http://localhost:7000\n"
     ]
    }
   ],
   "source": [
    "# Start the visualizer. The cell will output an HTTP link after the execution.\n",
    "# Click the link and a MeshCat tab should appear in your browser.\n",
    "meshcat = StartMeshcat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b502cc5811ed4c8c80d1ab16c03e243c",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Conveyor Belt Joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "941add2762ba4df2a7bf841eb68aca3f",
    "deepnote_cell_type": "code",
    "execution_context_id": "096e21bf-5fba-4fa9-a776-bea987b2f5d8",
    "execution_millis": 0,
    "execution_start": 1732475939993,
    "source_hash": "2c7923e0"
   },
   "outputs": [],
   "source": [
    "def AddRotatingDiskWithJoint(plant, disk_name):\n",
    "    \"\"\"\n",
    "    Adds a rotating disk to the MultibodyPlant and returns the revolute joint.\n",
    "    \"\"\"\n",
    "    # Create a new model instance for the disk\n",
    "    model_instance = plant.AddModelInstance(disk_name)\n",
    "    \n",
    "    # Create a rigid body for the disk within the specified model instance\n",
    "    disk_body = plant.AddRigidBody(disk_name, model_instance, SpatialInertia(\n",
    "        mass=1.0,  # 1 kg\n",
    "        p_PScm_E=np.array([0.0, 0.0, 0.0]),  # Center of mass at origin\n",
    "        G_SP_E=UnitInertia.SolidSphere(0.1)  # Radius of 0.1 m\n",
    "    ))\n",
    "    \n",
    "    # Add a revolute joint to allow rotation around the Z-axis\n",
    "    joint = plant.AddJoint(RevoluteJoint(\n",
    "        name=disk_name,\n",
    "        frame_on_parent=plant.world_frame(),\n",
    "        frame_on_child=disk_body.body_frame(),\n",
    "        axis=[0, 0, 1]  # Rotate around the Z-axis\n",
    "    ))\n",
    "    \n",
    "    return joint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b1e487f8067e405ba89ad92bc3c36df9",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Load The Masking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "437f7f01915a460fb15c7076a4a7b0ce",
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475940045,
    "source_hash": "b71b09c0"
   },
   "outputs": [],
   "source": [
    "if running_as_notebook:\n",
    "    model_file = \"clutter_maskrcnn_model.pt\"\n",
    "    if not os.path.exists(model_file):\n",
    "        urlretrieve(\n",
    "            \"https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_model.pt\",\n",
    "            model_file,\n",
    "        )\n",
    "# ycb = [\n",
    "#     \"003_cracker_box.sdf\",\n",
    "#     \"004_sugar_box.sdf\",\n",
    "#     \"005_tomato_soup_can.sdf\",\n",
    "#     \"006_mustard_bottle.sdf\",\n",
    "#     \"009_gelatin_box.sdf\",\n",
    "#     \"010_potted_meat_can.sdf\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "48bcaefa4e9d48db838dc92d0ff1ad9a",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "301fdf73-8afa-40db-a681-9791010114a3",
    "execution_millis": 754,
    "execution_start": 1732475940093,
    "is_code_hidden": true,
    "source_hash": "c0084ad2"
   },
   "outputs": [],
   "source": [
    "if running_as_notebook:\n",
    "\n",
    "    def get_instance_segmentation_model(num_classes):\n",
    "        # load an instance segmentation model pre-trained on COCO\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "            weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT, progress=False\n",
    "        )\n",
    "\n",
    "        # get the number of input features for the classifier\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # replace the pre-trained head with a new one\n",
    "        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "        # now get the number of input features for the mask classifier\n",
    "        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "        hidden_layer = 256\n",
    "        # and replace the mask predictor with a new one\n",
    "        model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "            in_features_mask, hidden_layer, num_classes\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    num_classes = 7\n",
    "    model = get_instance_segmentation_model(num_classes)\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model.load_state_dict(torch.load(\"clutter_maskrcnn_model.pt\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a12534efea074922b40fe45385cf2588",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [
     {
      "fromCodePoint": 8,
      "marks": {
       "bold": true
      },
      "toCodePoint": 30,
      "type": "marks"
     }
    ]
   },
   "source": [
    "Use the get_merged_masked_pcd function below to get the voxelized point cloud configuration from prediction data via the model and the image data from the cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "3e9faf2661244bdb919873f4e67ed357",
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 2,
    "execution_start": 1732475940924,
    "source_hash": "48d64d71"
   },
   "outputs": [],
   "source": [
    "def get_merged_masked_pcd(\n",
    "   predictions,\n",
    "   rgb_ims,\n",
    "   depth_ims,\n",
    "   project_depth_to_pC_funcs,\n",
    "   X_WCs,\n",
    "   label,\n",
    "   mask_threshold=150,\n",
    "):\n",
    "   \"\"\"\n",
    "   predictions: The output of the trained network (one for each camera)\n",
    "   rgb_ims: RGBA images from each camera\n",
    "   depth_ims: Depth images from each camera\n",
    "   project_depth_to_pC_funcs: Functions that perform the pinhole camera operations to convert pixels\n",
    "       into points. See the analogous function in problem 5.2 to see how to use it.\n",
    "   X_WCs: Poses of the cameras in the world frame\n",
    "   mask_threshold: Use this to determine which pixels in\n",
    "   \"\"\"\n",
    "\n",
    "   pcd = []\n",
    "   for prediction, rgb_im, depth_im, project_depth_to_pC_func, X_WC in zip(\n",
    "       predictions, rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs\n",
    "   ):\n",
    "       # These arrays aren't the same size as the correct outputs, but we're\n",
    "       # just initializing them to something valid for now.\n",
    "       spatial_points = np.zeros((3, 1))  # 3xN: (x,y,z) x Number of masked points\n",
    "       rgb_points = np.zeros((3, 1))  # 3xN: Color channels x Number of masked points\n",
    "\n",
    "       ######################################\n",
    "       # Your code here (populate spatial_points and rgb_points)\n",
    "\n",
    "       mask_idx = np.argmax(prediction[0][\"labels\"] == label)\n",
    "       mask = prediction[0][\"masks\"][mask_idx, 0]\n",
    "       idxs = mask > mask_threshold\n",
    "\n",
    "       # Mask pixels, then get points in camera frame\n",
    "       u_range = np.arange(depth_im.shape[0])\n",
    "       v_range = np.arange(depth_im.shape[1])\n",
    "       depth_v, depth_u = np.meshgrid(v_range, u_range)\n",
    "       depth_pnts = np.dstack([depth_u, depth_v, depth_im])\n",
    "       masked_depth_pnts = depth_pnts[idxs]\n",
    "       pC = np.expand_dims(project_depth_to_pC_func(masked_depth_pnts), 2)\n",
    "\n",
    "       # Convert to world frame\n",
    "       R = np.expand_dims(X_WC.rotation().matrix(), 0)\n",
    "       p = np.expand_dims(X_WC.translation(), [0, 2])\n",
    "       pC_in_world = np.matmul(R, pC) + p\n",
    "       spatial_points = np.squeeze(pC_in_world).T\n",
    "\n",
    "       color_pnts = rgb_im\n",
    "       color_pnts = color_pnts[idxs]\n",
    "       rgb_points = color_pnts[:, :3].T\n",
    "\n",
    "       ######################################\n",
    "\n",
    "       # You get an unhelpful RunTime error if your arrays are the wrong\n",
    "       # shape, so we'll check beforehand that they're the correct shapes.\n",
    "       assert (\n",
    "           len(spatial_points.shape) == 2\n",
    "       ), \"Spatial points is the wrong size -- should be 3 x N\"\n",
    "       assert (\n",
    "           spatial_points.shape[0] == 3\n",
    "       ), \"Spatial points is the wrong size -- should be 3 x N\"\n",
    "       assert (\n",
    "           len(rgb_points.shape) == 2\n",
    "       ), \"RGB points is the wrong size -- should be 3 x N\"\n",
    "       assert (\n",
    "           rgb_points.shape[0] == 3\n",
    "       ), \"RGB points is the wrong size -- should be 3 x N\"\n",
    "       assert rgb_points.shape[1] == spatial_points.shape[1]\n",
    "\n",
    "       N = spatial_points.shape[1]\n",
    "       pcd.append(PointCloud(N, Fields(BaseField.kXYZs | BaseField.kRGBs)))\n",
    "       pcd[-1].mutable_xyzs()[:] = spatial_points\n",
    "       pcd[-1].mutable_rgbs()[:] = rgb_points\n",
    "       # Estimate normals\n",
    "       pcd[-1].EstimateNormals(radius=0.1, num_closest=30)\n",
    "       # Flip normals toward camera\n",
    "       pcd[-1].FlipNormalsTowardPoint(X_WC.translation())\n",
    "\n",
    "   # Merge point clouds.\n",
    "   merged_pcd = Concatenate(pcd)\n",
    "\n",
    "   # Voxelize down-sample.  (Note that the normals still look reasonable)\n",
    "   return merged_pcd.VoxelizedDownSample(voxel_size=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "bb69b363b6c74d2785f2e9feb9a61645",
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475940969,
    "source_hash": "29ef0f2b"
   },
   "outputs": [],
   "source": [
    "# def GenerateAntipodalGraspCandidate(\n",
    "#     diagram,\n",
    "#     context,\n",
    "#     cloud,\n",
    "#     rng,\n",
    "#     wsg_body_index=None,\n",
    "#     scene_graph_system_name=\"scene_graph\",\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Picks a random point in the cloud, and aligns the robot finger with the\n",
    "#     normal of that pixel. The rotation around the normal axis is drawn from a\n",
    "#     uniform distribution over [min_roll, max_roll].\n",
    "\n",
    "#     Args:\n",
    "#         diagram: A diagram containing a MultibodyPlant+SceneGraph that contains\n",
    "#             a free body gripper and any obstacles in the environment that we\n",
    "#             want to check collisions against. It should not include the objects\n",
    "#             in the point cloud; those are handled separately.\n",
    "#         context: The diagram context.  All positions in the context will be\n",
    "#             held fixed *except* the gripper free body pose.\n",
    "#         cloud: a PointCloud in world coordinates which represents candidate\n",
    "#             grasps.\n",
    "#         rng: a np.random.default_rng()\n",
    "#         wsg_body_index: The body index of the gripper in plant.  If None,\n",
    "#             then a body named \"body\" will be searched for in the plant.\n",
    "\n",
    "#     Returns:\n",
    "#         cost: The grasp cost X_G: The grasp candidate\n",
    "#     \"\"\"\n",
    "#     station = diagram.GetSubsystemByName(\"station\")\n",
    "#     plant = station.GetSubsystemByName(\"plant\")\n",
    "#     plant_context = plant.GetMyMutableContextFromRoot(station.GetMyContextFromRoot(context))\n",
    "#     scene_graph = station.GetSubsystemByName(scene_graph_system_name)\n",
    "#     scene_graph.GetMyMutableContextFromRoot(station.GetMyContextFromRoot(context))\n",
    "#     if wsg_body_index:\n",
    "#         wsg = plant.get_body(wsg_body_index)\n",
    "#     else:\n",
    "#         wsg = plant.GetBodyByName(\"body\")\n",
    "#         wsg_body_index = wsg.index()\n",
    "\n",
    "#     if cloud.size() < 1:\n",
    "#         return np.inf, None\n",
    "\n",
    "#     index = rng.integers(0, cloud.size() - 1)\n",
    "\n",
    "#     # Use S for sample point/frame.\n",
    "#     p_WS = cloud.xyz(index)\n",
    "#     n_WS = cloud.normal(index)\n",
    "\n",
    "#     assert np.isclose(\n",
    "#         np.linalg.norm(n_WS), 1.0\n",
    "#     ), f\"Normal has magnitude: {np.linalg.norm(n_WS)}\"\n",
    "\n",
    "#     Gx = n_WS  # gripper x axis aligns with normal\n",
    "#     # make orthonormal y axis, aligned with world down\n",
    "#     y = np.array([0.0, 0.0, -1.0])\n",
    "#     if np.abs(np.dot(y, Gx)) < 1e-6:\n",
    "#         # normal was pointing straight down.  reject this sample.\n",
    "#         return np.inf, None\n",
    "\n",
    "#     Gy = y - np.dot(y, Gx) * Gx\n",
    "#     Gz = np.cross(Gx, Gy)\n",
    "#     R_WG = RotationMatrix(np.vstack((Gx, Gy, Gz)).T)\n",
    "#     p_GS_G = [0.054 - 0.01, 0.10625, 0]\n",
    "\n",
    "#     # Try orientations from the center out\n",
    "#     min_roll = -np.pi / 3.0\n",
    "#     max_roll = np.pi / 3.0\n",
    "#     alpha = np.array([0.5, 0.65, 0.35, 0.8, 0.2, 1.0, 0.0])\n",
    "#     for theta in min_roll + (max_roll - min_roll) * alpha:\n",
    "#         # Rotate the object in the hand by a random rotation (around the\n",
    "#         # normal).\n",
    "#         R_WG2 = R_WG.multiply(RotationMatrix.MakeXRotation(theta))\n",
    "\n",
    "#         # Use G for gripper frame.\n",
    "#         p_SG_W = -R_WG2.multiply(p_GS_G)\n",
    "#         p_WG = p_WS + p_SG_W\n",
    "\n",
    "#         X_G = RigidTransform(R_WG2, p_WG)\n",
    "#         plant.SetFreeBodyPose(plant_context, wsg, X_G)\n",
    "#         cost = GraspCandidateCost(diagram, context, cloud, adjust_X_G=True)\n",
    "#         X_G = plant.GetFreeBodyPose(plant_context, wsg)\n",
    "#         if np.isfinite(cost):\n",
    "#             return cost, X_G\n",
    "\n",
    "#     return np.inf, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "13e69eeb71bf4e31b8fcf6679f85d135",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": [
     {
      "fromCodePoint": 8,
      "marks": {
       "bold": true
      },
      "toCodePoint": 29,
      "type": "marks"
     }
    ]
   },
   "source": [
    "Use the find_antipodal_grasp function below to get the optimal gripper position based on the isolated point cloud of the object of interest. This function assumes no obstacles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute future cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_future_cloud(cloud):\n",
    "    X_BW = (RigidTransform(RotationMatrix.Identity(), [-5.15, 0, 0.8])).inverse()\n",
    "    cloud_WP = cloud.mutable_xyzs()\n",
    "    cloud_norms_WP = cloud.mutable_normals()\n",
    "    cloud_mut_BP = X_BW @ cloud_WP \n",
    "    cloud_norms_BP = X_BW @ cloud_norms_WP\n",
    "    duration = 6\n",
    "    d_theta = 0.01\n",
    "    rotate_BP =  RotationMatrix.MakeZRotation(d_theta * duration)\n",
    "    cloud_mut_BP = rotate_BP @ cloud_mut_BP\n",
    "    cloud_norms_BP = rotate_BP @ cloud_norms_BP\n",
    "    \n",
    "    cloud_norms_new, cloud_WP_new = X_BW.inverse() @ cloud_norms_BP,  X_BW.inverse() @ cloud_mut_BP\n",
    "\n",
    "    for i in range(cloud_WP.shape[1]):\n",
    "        cloud_WP[0][i], cloud_WP[1][i] = cloud_WP_new[0][i], cloud_WP_new[1][i]\n",
    "        # cloud_norms_WP[0][i], cloud_norms_WP[1][i] = cloud_norms_new[0][i], cloud_norms_new[1][i]\n",
    "    # return X_BW.inverse() @ cloud_norms_BP,  X_BW.inverse() @ cloud_mut_BP\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "7421b1c709a14c9ea8ab2a5edfc77249",
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475941013,
    "source_hash": "2bef07df"
   },
   "outputs": [],
   "source": [
    "def find_antipodal_grasp(environment_diagram, environment_context, cameras, label, predictions):\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    # Another diagram for the objects the robot \"knows about\": gripper, cameras, bins.  Think of this as the model in the robot's head.\n",
    "    builder = DiagramBuilder()\n",
    "    plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.001)\n",
    "    parser = Parser(plant)\n",
    "    ConfigureParser(parser)\n",
    "    parser.AddModelsFromUrl(\n",
    "        \"package://manipulation/schunk_wsg_50_welded_fingers.dmd.yaml\"\n",
    "    )\n",
    "    plant.Finalize()\n",
    "\n",
    "    params = MeshcatVisualizerParams()\n",
    "    params.prefix = \"planning\"\n",
    "    visualizer = MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat, params)\n",
    "    diagram = builder.Build()\n",
    "    context = diagram.CreateDefaultContext()\n",
    "    diagram.ForcedPublish(context)\n",
    "\n",
    "    for c in cameras:\n",
    "        c.compute_camera_data()\n",
    "    rgb_ims = [c.rgb_im for c in cameras]\n",
    "    depth_ims = [c.depth_im for c in cameras]\n",
    "    project_depth_to_pC_funcs = [c.project_depth_to_pC for c in cameras]\n",
    "    X_WCs = [c.X_WC for c in cameras]\n",
    "\n",
    "    cloud = get_merged_masked_pcd(\n",
    "        predictions, rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs, label\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"cloud past\", cloud.mutable_xyzs())\n",
    "    # cloud_xyzs = cloud.mutable_xyz()\n",
    "    # cloud_normals = cloud.mutable_normal()\n",
    "\n",
    "    compute_future_cloud(cloud)\n",
    "\n",
    "    meshcat.SetObject(\"Future_cloud\", cloud, point_size=0.01)\n",
    "\n",
    "    print(\"cloud_xyz_future\", cloud.mutable_xyzs())\n",
    "\n",
    "\n",
    "    plant_context = plant.GetMyContextFromRoot(context)\n",
    "    scene_graph_context = scene_graph.GetMyContextFromRoot(context)\n",
    "\n",
    "    min_cost = np.inf\n",
    "    best_X_G = RigidTransform()\n",
    "    for i in range(100):\n",
    "        cost, X_G = GenerateAntipodalGraspCandidate(diagram, context, cloud, rng)\n",
    "        if np.isfinite(cost) and cost < min_cost:\n",
    "            min_cost = cost\n",
    "            best_X_G = X_G\n",
    "\n",
    "    plant.SetFreeBodyPose(plant_context, plant.GetBodyByName(\"body\"), best_X_G)\n",
    "    diagram.ForcedPublish(context)\n",
    "    print(best_X_G)\n",
    "\n",
    "    return best_X_G\n",
    "\n",
    "\n",
    "# if running_as_notebook:\n",
    "#     find_antipodal_grasp(environment_diagram, environment_context, cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "d125781c82c2462ea2cbfef844af757d",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475941069,
    "is_code_hidden": false,
    "source_hash": "6886b870"
   },
   "outputs": [],
   "source": [
    "class CameraSystem:\n",
    "    def __init__(self, idx, meshcat, diagram, context):\n",
    "        self.idx = idx\n",
    "        self.context = context\n",
    "        self.diagram = diagram\n",
    "\n",
    "        # # Read images\n",
    "        # depth_im_read = (\n",
    "        #     diagram.GetOutputPort(\"camera{}_depth_image\".format(idx))\n",
    "        #     .Eval(context)\n",
    "        #     .data.squeeze()\n",
    "        # )\n",
    "        # self.depth_im = deepcopy(depth_im_read)\n",
    "        # self.depth_im[self.depth_im == np.inf] = 10.0\n",
    "        # self.rgb_im = (\n",
    "        #     diagram.GetOutputPort(\"camera{}_rgb_image\".format(idx)).Eval(context).data\n",
    "        # )\n",
    "\n",
    "        # # Get other info about the camera\n",
    "        # # cam = diagram.GetSubsystemByName(\"camera\" + str(idx))\n",
    "        # # cam_context = cam.GetMyMutableContextFromRoot(context)\n",
    "        # pose_subsystem = diagram.GetSubsystemByName(f\"camera{idx}.pose\")\n",
    "        # pose_subsystem_context = pose_subsystem.GetMyContextFromRoot(context)  # Get specific context for ExtractPose subsystem\n",
    "        # pose_output = pose_subsystem.GetOutputPort(\"pose\")\n",
    "        # pose_data = pose_output.Eval(pose_subsystem_context)\n",
    "        # #camera_frame = plant.GetFrameByName(\"base\", plant.GetModelInstanceByName(f\"camera{idx}_model\"))\n",
    "        # cam = diagram.GetSubsystemByName(\"station\").GetSubsystemByName(\"rgbd_sensor_camera\" + str(idx))\n",
    "        # self.X_WC = pose_data\n",
    "        # self.cam_info = cam.default_depth_render_camera().core().intrinsics()\n",
    "        # # self.fx = 525  # Focal length in pixels\n",
    "        # self.fy = 525  # Focal length in pixels\n",
    "        # self.cx = 320  # Principal point x-coordinate for a 640x480 image\n",
    "        # self.cy = 240  # Principal point y-coordinate for a 640x480 image\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def project_depth_to_pC(self, depth_pixel):\n",
    "        \"\"\"\n",
    "        project depth pixels to points in camera frame\n",
    "        using pinhole camera model\n",
    "        Input:\n",
    "            depth_pixels: numpy array of (nx3) or (3,)\n",
    "        Output:\n",
    "            pC: 3D point in camera frame, numpy array of (nx3)\n",
    "        \"\"\"\n",
    "        # switch u,v due to python convention\n",
    "        v = depth_pixel[:, 0]\n",
    "        u = depth_pixel[:, 1]\n",
    "        Z = depth_pixel[:, 2]\n",
    "        cx = self.cam_info.center_x()\n",
    "        cy = self.cam_info.center_y()\n",
    "        fx = self.cam_info.focal_x()\n",
    "        fy = self.cam_info.focal_y()\n",
    "        X = (u - cx) * Z / fx\n",
    "        Y = (v - cy) * Z / fy\n",
    "        pC = np.c_[X, Y, Z]\n",
    "        return pC\n",
    "    \n",
    "    def compute_camera_data(self):\n",
    "        # Read images\n",
    "        context = self.context\n",
    "        diagram = self.diagram\n",
    "        depth_im_read = (\n",
    "            diagram.GetOutputPort(\"camera{}_depth_image\".format(self.idx))\n",
    "            .Eval(context)\n",
    "            .data.squeeze()\n",
    "        )\n",
    "        self.depth_im = deepcopy(depth_im_read)\n",
    "        self.depth_im[self.depth_im == np.inf] = 10.0\n",
    "        self.rgb_im = (\n",
    "            diagram.GetOutputPort(\"camera{}_rgb_image\".format(self.idx)).Eval(context).data\n",
    "        )\n",
    "\n",
    "        # Get other info about the camera\n",
    "        # cam = diagram.GetSubsystemByName(\"camera\" + str(idx))\n",
    "        # cam_context = cam.GetMyMutableContextFromRoot(context)\n",
    "        pose_subsystem = diagram.GetSubsystemByName(f\"camera{self.idx}.pose\")\n",
    "        pose_subsystem_context = pose_subsystem.GetMyContextFromRoot(context)  # Get specific context for ExtractPose subsystem\n",
    "        pose_output = pose_subsystem.GetOutputPort(\"pose\")\n",
    "        pose_data = pose_output.Eval(pose_subsystem_context)\n",
    "        #camera_frame = plant.GetFrameByName(\"base\", plant.GetModelInstanceByName(f\"camera{idx}_model\"))\n",
    "        cam = diagram.GetSubsystemByName(\"station\").GetSubsystemByName(\"rgbd_sensor_camera\" + str(self.idx))\n",
    "        self.X_WC = pose_data\n",
    "        self.cam_info = cam.default_depth_render_camera().core().intrinsics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "900a6046d63241389d0e033f3e22b959",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "301fdf73-8afa-40db-a681-9791010114a3",
    "execution_millis": 1,
    "execution_start": 1732477037781,
    "is_code_hidden": false,
    "source_hash": "a55e7c8f"
   },
   "outputs": [],
   "source": [
    "# - add_model:\n",
    "#     name: table_top\n",
    "#     file: package://dummy_project/table_top.sdf\n",
    "\n",
    "# - add_weld:\n",
    "#     parent: world\n",
    "#     child: table_top::table_top_center\n",
    "\n",
    "# - add_model:\n",
    "#         name: dual_disk_model\n",
    "#         file: package://dummy_project/belt.sdf\n",
    "#         default_free_body_pose:\n",
    "#             disk_2:\n",
    "#                 translation: [-5.15, 0, 0.8]\n",
    "#                 rotation: !Rpy { deg: [0, 0, 0] }\n",
    "\n",
    "# - add_weld:\n",
    "#     parent: table_top::table_top_center\n",
    "#     child: dual_disk_model::disk_2_center\n",
    "#     X_PC:\n",
    "#         rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n",
    "#         translation: [-5.15, 0, 0.25]\n",
    "\n",
    "# - add_model:\n",
    "#         name: bin_model\n",
    "#         file: package://dummy_project/recyclable_bin.sdf\n",
    "#         default_free_body_pose:\n",
    "#             bin_base:\n",
    "#                 translation: [-4.65, -0.5, 0.8]\n",
    "#                 rotation: !Rpy { deg: [0, 0, 0] }\n",
    "\n",
    "# - add_weld:\n",
    "#     parent: table_top::table_top_center\n",
    "#     child: bin_model::bin_front_top_center\n",
    "#     X_PC:\n",
    "#         rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n",
    "#         translation: [0.35, 0.8, 0.25]\n",
    "\n",
    "# - add_model:\n",
    "#     name: non_bin_model\n",
    "#     file: package://dummy_project/non_recyclable_bin.sdf\n",
    "#     default_free_body_pose:\n",
    "#         bin_base:\n",
    "#             translation: [-5.65, 0.5, 0.8]\n",
    "#             rotation: !Rpy { deg: [0, 0, 0] }\n",
    "\n",
    "# - add_weld:\n",
    "#     parent: table_top::table_top_center\n",
    "#     child: non_bin_model::bin_front_top_center\n",
    "#     X_PC:\n",
    "#         rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n",
    "#         translation: [0.35, -0.8, 0.25]\n",
    "\n",
    "# - add_model:\n",
    "#     name: cube_model\n",
    "#     file: package://drake_models/ycb/004_sugar_box.sdf\n",
    "#     default_free_body_pose:\n",
    "#         base_link_sugar:\n",
    "#             translation: [-0.45, 0, 0.8]\n",
    "#             rotation: !Rpy { deg: [0, 0, 0] }\n",
    "\n",
    "scenario_data = \"\"\"\n",
    "directives:\n",
    "- add_model:\n",
    "    name: table_top\n",
    "    file: package://dummy_project/table_top.sdf\n",
    "\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: table_top::table_top_center\n",
    "\n",
    "- add_model:\n",
    "    name: dual_disk_model\n",
    "    file: package://dummy_project/belt.sdf\n",
    "    default_free_body_pose:\n",
    "        disk_2:\n",
    "            translation: [-5.15, 0, 0.8]\n",
    "            rotation: !Rpy { deg: [0, 0, 0] }\n",
    "\n",
    "- add_weld:\n",
    "    parent: table_top::table_top_center\n",
    "    child: dual_disk_model::disk_2_center\n",
    "    X_PC:\n",
    "        rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n",
    "        translation: [-5.15, 0, 0.25]\n",
    "\n",
    "- add_model:\n",
    "    name: bin_model\n",
    "    file: package://dummy_project/recyclable_bin.sdf\n",
    "    default_free_body_pose:\n",
    "        bin_base:\n",
    "            translation: [-4.65, -0.5, 0.8]\n",
    "            rotation: !Rpy { deg: [0, 0, 0] }\n",
    "\n",
    "- add_weld:\n",
    "    parent: table_top::table_top_center\n",
    "    child: bin_model::bin_front_top_center\n",
    "    X_PC:\n",
    "        rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n",
    "        translation: [0.35, 0.8, 0.25]\n",
    "\n",
    "- add_model:\n",
    "    name: non_bin_model\n",
    "    file: package://dummy_project/non_recyclable_bin.sdf\n",
    "    default_free_body_pose:\n",
    "        bin_base:\n",
    "            translation: [-5.65, 0.5, 0.8]\n",
    "            rotation: !Rpy { deg: [0, 0, 0] }\n",
    "\n",
    "- add_weld:\n",
    "    parent: table_top::table_top_center\n",
    "    child: non_bin_model::bin_front_top_center\n",
    "    X_PC:\n",
    "        rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n",
    "        translation: [0.35, -0.8, 0.25]\n",
    "\n",
    "- add_model:\n",
    "    name: cube_model\n",
    "    file: package://drake_models/ycb/004_sugar_box.sdf\n",
    "    default_free_body_pose:\n",
    "        base_link_sugar:\n",
    "            translation: [-0.7, -0.7, 0.35]\n",
    "            rotation: !Rpy { deg: [0, 0, 0] }\n",
    "\n",
    "- add_model:\n",
    "    name: iiwa\n",
    "    file: package://drake_models/iiwa_description/sdf/iiwa7_no_collision.sdf\n",
    "    default_joint_positions:\n",
    "        iiwa_joint_1: [-1.57]\n",
    "        iiwa_joint_2: [0.1]\n",
    "        iiwa_joint_3: [0]\n",
    "        iiwa_joint_4: [-1.2]\n",
    "        iiwa_joint_5: [0]\n",
    "        iiwa_joint_6: [1.6]\n",
    "        iiwa_joint_7: [0]\n",
    "\n",
    "- add_weld:\n",
    "    parent: world\n",
    "    child: iiwa::iiwa_link_0\n",
    "    X_PC:\n",
    "        translation: [0, 0, 0]\n",
    "        rotation: !Rpy { deg: [0, 0, -90] }\n",
    "\n",
    "- add_model:\n",
    "    name: wsg\n",
    "    file: package://drake_models/wsg_50_description/sdf/schunk_wsg_50_with_tip.sdf\n",
    "\n",
    "- add_weld:\n",
    "    parent: iiwa::iiwa_link_7\n",
    "    child: wsg::body\n",
    "    X_PC:\n",
    "        translation: [0, 0, 0.09]\n",
    "        rotation: !Rpy { deg: [90, 0, 90] }\n",
    "\n",
    "- add_frame:\n",
    "    name: cameras_frame\n",
    "    X_PF:\n",
    "        base_frame: world\n",
    "        translation: [-0.6, -0.21, 0.4]\n",
    "- add_frame:\n",
    "    name: camera0_frame\n",
    "    X_PF:\n",
    "        base_frame: cameras_frame\n",
    "        translation: [0.4, 0, 0.2]\n",
    "- add_frame:\n",
    "    name: camera1_frame\n",
    "    X_PF:\n",
    "        base_frame: cameras_frame\n",
    "        translation: [-0.4, 0, 0.2]\n",
    "\n",
    "- add_frame:\n",
    "    name: camera2_frame\n",
    "    X_PF:\n",
    "        base_frame: cameras_frame\n",
    "        translation: [0, 0.4, 0.2]\n",
    "\n",
    "\n",
    "- add_model:\n",
    "    name: camera0_model\n",
    "    file: package://manipulation/camera_box.sdf\n",
    "\n",
    "- add_weld:\n",
    "    parent: camera0_frame\n",
    "    child: camera0_model::base\n",
    "    X_PC:\n",
    "        rotation: !Rpy { deg: [-120, 0, 90] }\n",
    "\n",
    "- add_model:\n",
    "    name: camera1_model\n",
    "    file: package://manipulation/camera_box.sdf\n",
    "\n",
    "- add_weld:\n",
    "    parent: camera1_frame\n",
    "    child: camera1_model::base\n",
    "    X_PC:\n",
    "        rotation: !Rpy { deg: [-120, 0, -90] }\n",
    "\n",
    "- add_model:\n",
    "    name: camera2_model\n",
    "    file: package://manipulation/camera_box.sdf\n",
    "\n",
    "- add_weld:\n",
    "    parent: camera2_frame\n",
    "    child: camera2_model::base\n",
    "    X_PC:\n",
    "        rotation: !Rpy { deg: [-120, 0, 180] }\n",
    "plant_config:\n",
    "    time_step: 1e-2\n",
    "    contact_model: \"hydroelastic_with_fallback\"\n",
    "    discrete_contact_approximation: \"sap\"\n",
    "\n",
    "\n",
    "model_drivers:\n",
    "    iiwa: !IiwaDriver\n",
    "        control_mode: position_only\n",
    "        hand_model_name: wsg\n",
    "    wsg: !SchunkWsgDriver {}\n",
    "    dual_disk_model: !JointStiffnessDriver\n",
    "        gains:\n",
    "            disk_joint:\n",
    "                kp: 600\n",
    "                kd: 120\n",
    "cameras:\n",
    "    main_camera:\n",
    "        name: camera0\n",
    "        depth: True\n",
    "        X_PB:\n",
    "            base_frame: camera0_model::base\n",
    "    secondary_camera:\n",
    "        name: camera1\n",
    "        depth: True\n",
    "        X_PB:\n",
    "            base_frame: camera1_model::base\n",
    "    third_camera:\n",
    "        name: camera2\n",
    "        depth: True\n",
    "        X_PB:\n",
    "            base_frame: camera2_model::base\n",
    "         \n",
    "\"\"\"\n",
    "\n",
    "class IIWA_ARM:\n",
    "    def __init__(self, builder, scenario, meshcat):\n",
    "        self.builder = builder\n",
    "        self.meshcat = meshcat\n",
    "        self.station = self.builder.AddSystem(\n",
    "            MakeHardwareStation(\n",
    "                scenario, \n",
    "                self.meshcat, \n",
    "                package_xmls=[os.getcwd() + \"/package.xml\"])\n",
    "        )\n",
    "        self.plant = self.station.GetSubsystemByName(\"plant\")\n",
    "\n",
    "        self.max_tries = 10\n",
    "\n",
    "        self.iiwa_model_instance = self.plant.GetModelInstanceByName(\"iiwa\")\n",
    "\n",
    "        self.gripper_frame = self.plant.GetFrameByName(\"body\")\n",
    "        self.world_frame = self.plant.world_frame()\n",
    "\n",
    "        self.visualizer = MeshcatVisualizer.AddToBuilder(\n",
    "            self.builder, self.station.GetOutputPort(\"query_object\"), self.meshcat\n",
    "        )\n",
    "\n",
    "        self.context = self.station.CreateDefaultContext()\n",
    "        self.station_context = self.station.GetMyMutableContextFromRoot(self.context)\n",
    "        self.plant_context = self.plant.GetMyMutableContextFromRoot(self.context)\n",
    "\n",
    "    def get_station(self):\n",
    "        return self.station\n",
    "    \n",
    "    def get_plant(self):\n",
    "        return self.plant\n",
    "\n",
    "    def num_input_ports(self):\n",
    "        return self.station.num_input_ports()\n",
    "\n",
    "    def visualize_frame(self, name, X_WF, length=0.15, radius=0.006):\n",
    "        AddMeshcatTriad(\n",
    "            self.meshcat, name, length=length, radius=radius, X_PT=X_WF\n",
    "        )\n",
    "    \n",
    "    def visualize_gripper_at_target(self, target_joint_positions):\n",
    "        context = self.plant.CreateDefaultContext()\n",
    "        self.plant.SetPositions(context, self.iiwa_model_instance, target_joint_positions)\n",
    "\n",
    "        X_WG_target = self.plant.CalcRelativeTransform(\n",
    "            context, frame_A=self.world_frame, frame_B=self.gripper_frame\n",
    "        )\n",
    "        self.visualize_frame(\"gripper_target_pose\", X_WG_target)\n",
    "\n",
    "    def clear_all_triads(self):\n",
    "        self.meshcat.Delete()\n",
    "\n",
    "    def get_current_gripper_joint_position(self):\n",
    "        return self.plant.GetPositions(self.plant_context, self.plant.GetModelInstanceByName(\"iiwa\"))\n",
    "\n",
    "    def get_X_WG(self):\n",
    "        return self.plant.CalcRelativeTransform(\n",
    "            self.plant_context, self.world_frame, self.gripper_frame\n",
    "        )\n",
    "\n",
    "    def get_X_WTarget(self, translation):\n",
    "        X_WG = self.get_X_WG()\n",
    "        rotation_matrix = RotationMatrix.Identity()\n",
    "        X_GTarget = RigidTransform(\n",
    "            rotation_matrix, \n",
    "            translation\n",
    "        )\n",
    "\n",
    "        X_WTarget = X_WG @ X_GTarget\n",
    "        return X_WTarget\n",
    "\n",
    "    def find_joint_positions(self, X_WTarget):\n",
    "        context = self.plant.CreateDefaultContext()\n",
    "        end_effector_frame = self.plant.GetFrameByName('body')\n",
    "\n",
    "        ik = InverseKinematics(self.plant, context)\n",
    "        ik.AddPositionConstraint(\n",
    "            frameB=end_effector_frame,\n",
    "            p_BQ=np.zeros((3, 1)),\n",
    "            frameA=self.plant.world_frame(),\n",
    "            p_AQ_lower=X_WTarget.translation().reshape((3, 1)) - 0.01,\n",
    "            p_AQ_upper=X_WTarget.translation().reshape((3, 1)) + 0.01\n",
    "        )\n",
    "\n",
    "        ik.AddOrientationConstraint(\n",
    "            frameAbar=self.plant.world_frame(),\n",
    "            R_AbarA=X_WTarget.rotation(),\n",
    "            frameBbar=end_effector_frame,\n",
    "            R_BbarB=RotationMatrix(),\n",
    "            theta_bound=0.01 * np.pi\n",
    "        )\n",
    "\n",
    "        last_joint_positions = None\n",
    "        for _ in range (self.max_tries):\n",
    "            q0 = self.plant.GetPositions(context).copy()\n",
    "            ik.prog().SetInitialGuess(ik.q(), q0)\n",
    "\n",
    "            # Solve the inverse kinematics problem\n",
    "            result = Solve(ik.prog())\n",
    "            # print(result.GetSolution(ik.q()[1:8]))\n",
    "            # last_joint_positions = result.GetSolution(ik.q()[:7])\n",
    "            last_joint_positions = result.GetSolution(ik.q()[1:8])\n",
    "            if result.is_success():\n",
    "                print(\"Success!\")\n",
    "                joint_positions = last_joint_positions\n",
    "                return joint_positions\n",
    "\n",
    "        return last_joint_positions\n",
    "\n",
    "        # # Solve the inverse kinematics problem\n",
    "        # result = Solve(ik.prog())\n",
    "        # joint_positions = result.GetSolution(ik.q()[:7])\n",
    "        # return joint_positions\n",
    "\n",
    "    def move_arm(self, trajectory_source, wsg_src = None):\n",
    "        self.builder.AddSystem(trajectory_source)\n",
    "\n",
    "        iiwa_position_input = self.station.GetInputPort(\"iiwa.position\")\n",
    "        self.builder.Connect(trajectory_source.get_output_port(), iiwa_position_input)\n",
    "\n",
    "        wsg_input = self.station.GetInputPort(\"wsg.position\")\n",
    "\n",
    "        if wsg_src is None:\n",
    "            wsg_src = ConstantVectorSource(np.zeros(wsg_input.size()))\n",
    "\n",
    "        self.builder.AddSystem(wsg_src)\n",
    "        self.builder.Connect(wsg_src.get_output_port(), wsg_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a4d952432be44f3ebe232d7fbbfb6ced",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Conveyor belt motion- Leaf System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "d68c4337a56a44c48be629cbfc370834",
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475941165,
    "source_hash": "d109b39"
   },
   "outputs": [],
   "source": [
    "# from pydrake.systems.framework import DiagramBuilder, LeafSystem\n",
    "# from pydrake.systems.primitives import ConstantVectorSource, Adder\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # Custom LeafSystem to apply torque at a specific joint index\n",
    "# class TorqueMappingSystem(LeafSystem):\n",
    "#     def __init__(self, joint_index, size):\n",
    "#         super().__init__()\n",
    "#         self.joint_index = joint_index\n",
    "#         self.size = size\n",
    "\n",
    "#         # Declare the input and output ports\n",
    "#         self.DeclareVectorInputPort(\"input_torque\", 1)  # Single torque value\n",
    "#         self.DeclareVectorOutputPort(\n",
    "#             \"output_torque_vector\",\n",
    "#             size,\n",
    "#             self.map_torque_to_vector\n",
    "#         )\n",
    "\n",
    "#     def map_torque_to_vector(self, context, output):\n",
    "#         torque = self.get_input_port().Eval(context)[0]\n",
    "#         torque_vector = np.zeros(self.size)\n",
    "#         torque_vector[self.joint_index] = torque\n",
    "#         # output[:] = torque_vector\n",
    "        \n",
    "#         output.SetFromVector(torque_vector)\n",
    "\n",
    "\n",
    "# def create_scene_with_motion(sim_time_step):\n",
    "#     # Clean up the Meshcat instance.\n",
    "#     meshcat.Delete()\n",
    "#     meshcat.DeleteAddedControls()\n",
    "\n",
    "#     builder = DiagramBuilder()\n",
    "#     # plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=sim_time_step)\n",
    "#     # parser = Parser(plant)\n",
    "\n",
    "#     # # Load the belt\n",
    "#     # belt_model = parser.AddModels(belt_sdf_path)\n",
    "#     # belt_frame = plant.GetFrameByName(\"disk_2_center\", belt_model[0])\n",
    "#     # plant.WeldFrames(\n",
    "#     #     plant.world_frame(),\n",
    "#     #     belt_frame,\n",
    "#     #     RigidTransform(RollPitchYaw(0, 0, 0), [0, 0, 0.25])\n",
    "#     # )\n",
    "\n",
    "#     # # Load the iiwa robot arm\n",
    "#     # iiwa_model = parser.AddModels(url=\"package://drake_models/iiwa_description/sdf/iiwa7_with_box_collision.sdf\")\n",
    "#     # iiwa_frame = plant.GetFrameByName(\"iiwa_link_0\", iiwa_model[0])\n",
    "#     # plant.WeldFrames(\n",
    "#     #     plant.world_frame(),\n",
    "#     #     iiwa_frame,\n",
    "#     #     RigidTransform(RollPitchYaw(0, 0, 0), [5.15, 0, 0.0])\n",
    "#     # )\n",
    "\n",
    "#     # # Add a rotating disk\n",
    "#     # # disk_1_joint = AddRotatingDiskWithJoint(plant, \"disk_1\")\n",
    "#     # # disk_1_joint_index = disk_1_joint.index()\n",
    "\n",
    "#     # # Finalize the plant\n",
    "#     # plant.Finalize()\n",
    "    \n",
    "#     scenario = LoadScenario(data=model_directive)\n",
    "#     station = builder.AddSystem(MakeHardwareStation(\n",
    "#         scenario, meshcat, package_xmls=[os.getcwd() + \"/package.xml\"]\n",
    "#     ))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Desired constant disk velocity\n",
    "#     desired_velocity = 0.1  # Constant velocity in radians/second \n",
    "\n",
    "#     # Create a ConstantVectorSource for velocity\n",
    "#     velocity_source = builder.AddSystem(ConstantVectorSource([desired_velocity]))\n",
    "\n",
    "#     # Create an Integrator to compute the position from velocity\n",
    "#     position_integrator = builder.AddSystem(Integrator(1))  # Single-dimensional integrator\n",
    "\n",
    "#     # Connect the velocity source to the integrator\n",
    "#     builder.Connect(velocity_source.get_output_port(), position_integrator.get_input_port())\n",
    "\n",
    "#     # Concatenate position and velocity into the desired_state vector\n",
    "#     multiplexer = builder.AddSystem(Multiplexer(num_scalar_inputs=2))\n",
    "#     builder.Connect(position_integrator.get_output_port(), multiplexer.get_input_port(0))  # Position\n",
    "#     builder.Connect(velocity_source.get_output_port(), multiplexer.get_input_port(1))     # Velocity\n",
    "\n",
    "#     # Connect the concatenated desired_state to your joint controller\n",
    "#     # (Assume 'joint_controller' is already defined and added to the builder)\n",
    "#     builder.Connect(multiplexer.get_output_port(), station.GetInputPort(\"dual_disk_model.desired_state\"))\n",
    "\n",
    "#     # Visualization\n",
    "#     # AddDefaultVisualization(builder=builder, meshcat=meshcat)\n",
    "\n",
    "#     # Build the diagram\n",
    "#     diagram = builder.Build()\n",
    "#     return diagram\n",
    "\n",
    "\n",
    "# def run_simulation_with_motion(sim_time_step):\n",
    "#     diagram = create_scene_with_motion(sim_time_step)\n",
    "#     simulator = Simulator(diagram)\n",
    "#     simulator.set_target_realtime_rate(1.0)\n",
    "    \n",
    "#     context = simulator.get_mutable_context()\n",
    "#     # Fix constant position for the iiwa arm\n",
    "#     iiwa_position = [0, 0, 0, -1.57, 0, 1.57, 0]\n",
    "#     station = diagram.GetSubsystemByName(\"station\")\n",
    "#     station_context = station.GetMyMutableContextFromRoot(context)\n",
    "#     station.GetInputPort(\"iiwa.position\").FixValue(station_context, np.array(iiwa_position))\n",
    "    \n",
    "#     meshcat.StartRecording()\n",
    "#     simulator.AdvanceTo(120.0)  # Run simulation for 2 seconds\n",
    "#     meshcat.PublishRecording()\n",
    "\n",
    "\n",
    "# # Run the simulation with motion\n",
    "# run_simulation_with_motion(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "0310499d161e49cfbaf29ec174697707",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475941213,
    "is_code_hidden": false,
    "source_hash": "28ee8414"
   },
   "outputs": [],
   "source": [
    "class TrajOptPlanner:\n",
    "    def __init__(self, num_time_samples=20):\n",
    "        self.num_time_samples = num_time_samples\n",
    "\n",
    "    def plan(self, start, goal):\n",
    "        prog = MathematicalProgram()\n",
    "        n = len(start)\n",
    "        h = 1.0 / (self.num_time_samples - 1)\n",
    "\n",
    "        # Decision variables for joint positions and velocities\n",
    "        q = prog.NewContinuousVariables(self.num_time_samples, n, \"q\")\n",
    "        v = prog.NewContinuousVariables(self.num_time_samples, n, \"v\")\n",
    "\n",
    "        # Initial and final state constraints\n",
    "        prog.AddBoundingBoxConstraint(start, start, q[0])\n",
    "        prog.AddBoundingBoxConstraint(goal, goal, q[-1])\n",
    "        prog.AddBoundingBoxConstraint(np.zeros(n), np.zeros(n), v[0])\n",
    "        prog.AddBoundingBoxConstraint(np.zeros(n), np.zeros(n), v[-1])\n",
    "\n",
    "        for i in range(self.num_time_samples - 1):\n",
    "            q_next = q[i] + h * v[i]\n",
    "            for j in range(n):\n",
    "                prog.AddLinearEqualityConstraint(q_next[j] == q[i + 1][j])\n",
    "\n",
    "        R = 10  # Cost on input \"effort\".\n",
    "        for i in range(self.num_time_samples - 1):\n",
    "            u = (v[i + 1] - v[i]) / h\n",
    "            prog.AddQuadraticCost(R * u.dot(u))\n",
    "\n",
    "        result = Solve(prog)\n",
    "        if not result.is_success():\n",
    "            raise RuntimeError(\"Trajectory optimization failed\")\n",
    "\n",
    "        q_trajectory = result.GetSolution(q)\n",
    "        v_trajectory = result.GetSolution(v)\n",
    "        times = np.linspace(0, 3, self.num_time_samples)\n",
    "\n",
    "        return times, q_trajectory, v_trajectory\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "28970234c7924302b064870f7d561a58",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475941261,
    "is_code_hidden": false,
    "source_hash": "cb2e33bd"
   },
   "outputs": [],
   "source": [
    "class Desired_Pose:\n",
    "    def __init__(self, desired_translation, desired_rotation, desired_wsg = 0.0):\n",
    "        self.desired_translation = desired_translation\n",
    "        self.desired_rotation = desired_rotation\n",
    "        self.desired_wsg = desired_wsg\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Desired_Pose({self.desired_translation}, {self.desired_rotation}, {self.desired_wsg})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "332dd10bca95479787ffb514d77f4d0c",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475941305,
    "is_code_hidden": false,
    "source_hash": "b3fa913d"
   },
   "outputs": [],
   "source": [
    "from pydrake.all import (\n",
    "    LeafSystem,\n",
    "    AbstractValue,\n",
    "    BasicVector,\n",
    "    DiscreteUpdateEvent,\n",
    "    PiecewisePose\n",
    ")\n",
    "\n",
    "class DynamicTrajectorySource(LeafSystem):\n",
    "    def __init__(self, initial_trajectory):\n",
    "        LeafSystem.__init__(self)\n",
    "        self.DeclareVectorOutputPort(\"trajectory_output\", initial_trajectory.rows(), self.CalcOutput)\n",
    "        self.trajectory = initial_trajectory\n",
    "\n",
    "    def CalcOutput(self, context, output):\n",
    "        time = context.get_time()\n",
    "        # print(self.trajectory.value(time))\n",
    "        output.SetFromVector(self.trajectory.value(time))\n",
    "\n",
    "    def update_trajectory(self, new_trajectory):\n",
    "        self.trajectory = new_trajectory\n",
    "\n",
    "def update_trajectory(trajectory_source: DynamicTrajectorySource, new_trajectory: PiecewisePolynomial):\n",
    "    trajectory_source.update_trajectory(new_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "0dfd6fac31434f2291e65649584ebcc4",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475941357,
    "is_code_hidden": false,
    "source_hash": "16ff694b"
   },
   "outputs": [],
   "source": [
    "class SelfMadeArmDriver:\n",
    "    def __init__(\n",
    "        self, \n",
    "        IIWA: IIWA_ARM, \n",
    "        planner: TrajOptPlanner, \n",
    "        dynamic_joint_trajectory_source: DynamicTrajectorySource,\n",
    "        dynamic_wsg_trajectory_source: DynamicTrajectorySource\n",
    "    ):\n",
    "        self.iiwa = IIWA\n",
    "        self.planner = planner\n",
    "        self.dynamic_joint_trajectory_source = dynamic_joint_trajectory_source\n",
    "        self.dynamic_wsg_trajectory_source = dynamic_wsg_trajectory_source\n",
    "\n",
    "    def set_desired_poses_and_follow(self, desired_poses: list[Desired_Pose], start_time = 0.0, dynamic_joint_trajectory_source = None):\n",
    "        pose_positions = [self.iiwa.get_X_WG()]\n",
    "        # print(desired_poses)\n",
    "\n",
    "        # self.iiwa.clear_all_triads()\n",
    "        self.iiwa.visualize_frame(\"current_gripper\", pose_positions[0])\n",
    "        desired_rotation = [0]\n",
    "        desired_wsg_grip = [0.0]\n",
    "        for i, d in enumerate(desired_poses):\n",
    "            # x_w_target = self.iiwa.get_X_WTarget(d.desired_translation)\n",
    "            x_w_target = RigidTransform(RotationMatrix().Identity(), d.desired_translation)\n",
    "            self.iiwa.visualize_frame(f\"gripper_target_{i}\", x_w_target)\n",
    "            pose_positions.append(x_w_target)\n",
    "            desired_rotation.append(d.desired_rotation)\n",
    "            desired_wsg_grip.append(d.desired_wsg)\n",
    "\n",
    "        # print(desired_wsg_grip)\n",
    "        return self.follow_positions(pose_positions, desired_rotation, desired_wsg_grip, start_time)\n",
    "\n",
    "\n",
    "    def generate_grip_values(self, times, initial_wsg_grip, desired_grip, transition_steps = 7):\n",
    "        num_steps = len(times)\n",
    "        grip_values = [initial_wsg_grip] * (num_steps - transition_steps)\n",
    "        transition_values = [initial_wsg_grip + (desired_grip - initial_wsg_grip) * (i / (transition_steps - 1)) for i in range(transition_steps)]\n",
    "        grip_values.extend(transition_values)\n",
    "        return grip_values\n",
    "\n",
    "    def follow_positions_works(\n",
    "        self, \n",
    "        pose_positions: list[RigidTransform], \n",
    "        desired_rotation: list[float], \n",
    "        desired_wsg_grips: list[float],\n",
    "        start_time: float\n",
    "    ):\n",
    "        times, joint_positions, wsg_positions = [], [], []\n",
    "\n",
    "        # print(desired_wsg_grips)\n",
    "\n",
    "        for i, pose in enumerate(pose_positions[:-1]):\n",
    "            start_joint_positions = self.iiwa.find_joint_positions(pose)\n",
    "\n",
    "            start_joint_positions[-1] = desired_rotation[i]\n",
    "\n",
    "            end_joint_positions = self.iiwa.find_joint_positions(pose_positions[i + 1])\n",
    "            end_joint_positions[-1] = desired_rotation[i + 1]\n",
    "\n",
    "            t, q, _ = self.planner.plan(start_joint_positions, end_joint_positions)\n",
    "\n",
    "            wsg_gripper_position = self.generate_grip_values(\n",
    "                t, \n",
    "                desired_wsg_grips[i], \n",
    "                desired_wsg_grips[i+1]\n",
    "            )\n",
    "\n",
    "            times.append(t)\n",
    "            joint_positions.append(q)\n",
    "            wsg_positions.append(wsg_gripper_position)\n",
    "            # print(q[-1])\n",
    "\n",
    "        total_t, total_q, grip_values = self.combine_all_trajectories(times, joint_positions, wsg_positions, start_time)\n",
    "        total_t, total_q= np.array(total_t), np.array(total_q).T\n",
    "        trajectory = PiecewisePolynomial.FirstOrderHold(total_t, total_q)\n",
    "\n",
    "        total_wsg = [np.array([[value]]) for value in grip_values]\n",
    "        wsg_trajectory = PiecewisePolynomial.FirstOrderHold(total_t, total_wsg)\n",
    "\n",
    "        trajectory_source = TrajectorySource(trajectory)\n",
    "        wsg_trajectory_source = TrajectorySource(wsg_trajectory)\n",
    "\n",
    "        self.iiwa.move_arm(trajectory_source, wsg_trajectory_source)\n",
    "\n",
    "        return trajectory.end_time()\n",
    "        \n",
    "    def follow_positions(\n",
    "        self, \n",
    "        pose_positions: list[RigidTransform], \n",
    "        desired_rotation: list[float], \n",
    "        desired_wsg_grips: list[float],\n",
    "        start_time: float\n",
    "    ):\n",
    "        times, joint_positions, wsg_positions = [], [], []\n",
    "\n",
    "        # print(desired_wsg_grips)\n",
    "\n",
    "        for i, pose in enumerate(pose_positions[:-1]):\n",
    "            start_joint_positions = self.iiwa.find_joint_positions(pose)\n",
    "\n",
    "            start_joint_positions[-1] = desired_rotation[i]\n",
    "\n",
    "            end_joint_positions = self.iiwa.find_joint_positions(pose_positions[i + 1])\n",
    "            end_joint_positions[-1] = desired_rotation[i + 1]\n",
    "\n",
    "            # print(start_joint_positions, end_joint_positions)\n",
    "            # print()\n",
    "            t, q, _ = self.planner.plan(start_joint_positions, end_joint_positions)\n",
    "\n",
    "            wsg_gripper_position = self.generate_grip_values(\n",
    "                t, \n",
    "                desired_wsg_grips[i], \n",
    "                desired_wsg_grips[i+1]\n",
    "            )\n",
    "\n",
    "            times.append(t)\n",
    "            joint_positions.append(q)\n",
    "            wsg_positions.append(wsg_gripper_position)\n",
    "            # print(q[-1])\n",
    "\n",
    "        total_t, total_q, grip_values = self.combine_all_trajectories(times, joint_positions, wsg_positions, start_time)\n",
    "        # for t, q in zip(total_t, total_q):\n",
    "        #     print(f\"{t}, {q}\")\n",
    "        \n",
    "        total_t, total_q= np.array(total_t), np.array(total_q).T\n",
    "        trajectory = PiecewisePolynomial.FirstOrderHold(total_t, total_q)\n",
    "\n",
    "        total_wsg = [np.array([[value]]) for value in grip_values]\n",
    "        wsg_trajectory = PiecewisePolynomial.FirstOrderHold(total_t, total_wsg)\n",
    "\n",
    "        update_trajectory(self.dynamic_joint_trajectory_source, trajectory)\n",
    "        update_trajectory(self.dynamic_wsg_trajectory_source, wsg_trajectory)\n",
    "\n",
    "        return trajectory.end_time()\n",
    "\n",
    "    def combine_all_trajectories(self, times, joint_positions, wsg_positions,start_time: float):\n",
    "        t_res, q_res, wsg_res = times[0] + start_time, joint_positions[0], wsg_positions[0]\n",
    "\n",
    "        for i in range (1, len(times)):\n",
    "            next_time = times[i] + t_res[-1]\n",
    "            t_res = np.concatenate([t_res, next_time[1:]])\n",
    "            q_res = np.concatenate([q_res, joint_positions[i][1:]])\n",
    "            wsg_res = np.concatenate([wsg_res, wsg_positions[i][1:]])\n",
    "\n",
    "        # for t, q in zip(t_res, q_res):\n",
    "        #     print(f\"{t}, {q}\")\n",
    "\n",
    "        return t_res, q_res, wsg_res\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cell_id": "4dfaf08702b74d9e81d536c17e3ab96b",
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 0,
    "execution_start": 1732475941413,
    "source_hash": "fc6cff9b"
   },
   "outputs": [],
   "source": [
    "class PerceptionModule:\n",
    "    def __init__ (self, diagram, context):\n",
    "        self.cameras = [CameraSystem(i, meshcat, diagram, context) for i in range(3)]\n",
    "        self.diagram = diagram\n",
    "        self.context = context\n",
    "    def update_cameras(self):\n",
    "        for c in self.cameras:\n",
    "            c.compute_camera_data()\n",
    "    def valid_prediction(self, predictions):\n",
    "        return (predictions[0][0][\"boxes\"].size != 0)\n",
    "    def get_camera_data(self):\n",
    "        cameras = self.cameras\n",
    "        rgb_ims = [c.rgb_im for c in cameras]\n",
    "        depth_ims = [c.depth_im for c in cameras]\n",
    "        project_depth_to_pC_funcs = [c.project_depth_to_pC for c in cameras]\n",
    "        X_WCs = [c.X_WC for c in cameras]\n",
    "        return rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs\n",
    "    def predict_mask(self, current_time):\n",
    "        cameras = self.cameras\n",
    "        if running_as_notebook and current_time == 8.0:\n",
    "            with torch.no_grad():\n",
    "                predictions = []\n",
    "                predictions.append(\n",
    "                    model([Tf.to_tensor(cameras[0].rgb_im[:, :, :3]).to(device)])\n",
    "                )\n",
    "                predictions.append(\n",
    "                    model([Tf.to_tensor(cameras[1].rgb_im[:, :, :3]).to(device)])\n",
    "                )\n",
    "                predictions.append(\n",
    "                    model([Tf.to_tensor(cameras[2].rgb_im[:, :, :3]).to(device)])\n",
    "                )\n",
    "                for i in range(3):\n",
    "                    for k in predictions[i][0].keys():\n",
    "                        if k == \"masks\":\n",
    "                            predictions[i][0][k] = (\n",
    "                                predictions[i][0][k].mul(255).byte().cpu().numpy()\n",
    "                            )\n",
    "                        else:\n",
    "                            predictions[i][0][k] = predictions[i][0][k].cpu().numpy()\n",
    "           # print(predictions)\n",
    "            return predictions\n",
    "        else:\n",
    "            return [[{\"boxes\": np.empty((0, 4), dtype=np.float32)}]]\n",
    "    def empty_mask(self):\n",
    "        return [[{\"boxes\": np.empty((0, 4), dtype=np.float32)}]]\n",
    "    def get_antipodal_grasps(self, diagram, context, predictions, X_WG_curr):\n",
    "        label = mode([predictions[0][0]['labels'][0], predictions[1][0]['labels'][0], predictions[2][0]['labels'][0]])[0]\n",
    "\n",
    "        pcd = get_merged_masked_pcd(\n",
    "            predictions, *self.get_camera_data(), label\n",
    "        )    \n",
    "\n",
    "        antipodal_grasp = find_antipodal_grasp(self.diagram, self.context, self.cameras, label, predictions)\n",
    "        X_WG = antipodal_grasp\n",
    "        translation = X_WG.translation()\n",
    "        # translation_1 = translation + np.array([0, 0, 0.2])\n",
    "        # translation_2 = translation + np.array([0, 0, 0.4])\n",
    "        rotation = (X_WG_curr.inverse() @ X_WG).rotation().ToRollPitchYaw().yaw_angle()\n",
    "        rotation = abs(rotation)\n",
    "        print(rotation)\n",
    "        return  translation, rotation\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "831e7fb05588410d9a95a5f1ed16ce22",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# State Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "35f3699df326407da49229f503794d62",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "e45dcd8c-be68-451f-8661-b06dcde98e94",
    "execution_millis": 1,
    "execution_start": 1732475941473,
    "is_code_hidden": false,
    "source_hash": "8fe18505"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydrake.all import (\n",
    "    LeafSystem\n",
    ")\n",
    "\n",
    "from enum import (\n",
    "    Enum\n",
    ")\n",
    "\n",
    "class STATES(int, Enum):\n",
    "    IDLE = 0\n",
    "    FOLLOW = 1\n",
    "    \n",
    "\n",
    "class StateMachine(LeafSystem):\n",
    "    def __init__(\n",
    "        self,\n",
    "        builder: DiagramBuilder,\n",
    "        meshcat: MeshcatVisualizer,\n",
    "    ):\n",
    "        LeafSystem.__init__(self)\n",
    "        self.builder = builder\n",
    "        self.meshcat = meshcat\n",
    "        self.DeclarePeriodicDiscreteUpdateEvent(0.1, 0.0, self.Update)\n",
    "\n",
    "        self.iiwa = IIWA_ARM(builder, scenario, meshcat)\n",
    "        self.station = self.iiwa.get_station()\n",
    "        self.plant = self.iiwa.get_plant()\n",
    "        self.planner = TrajOptPlanner()\n",
    "        self.antipodal_calculated = False\n",
    "\n",
    "        self.start_time = 0\n",
    "        self.state = STATES.IDLE\n",
    "        self.initial_joint_position = self.iiwa.get_current_gripper_joint_position()\n",
    "        self.predictions = [[{\"boxes\": np.empty((0, 4), dtype=np.float32)}]]\n",
    "\n",
    "        self.initial_gripper_pose = self.iiwa.get_X_WG()\n",
    "        #print(self.initial_gripper_pose)\n",
    "\n",
    "        # Setup initial trajectory\n",
    "        times = np.array([0, 0.1])\n",
    "        positions = np.tile(self.initial_joint_position, (2, 1)).T\n",
    "        self.initial_trajectory = PiecewisePolynomial.FirstOrderHold(times, positions)\n",
    "        self.iiwa_position_input = self.station.GetInputPort(\"iiwa.position\")\n",
    "        self.dynamic_trajectory_source = self.builder.AddSystem(DynamicTrajectorySource(self.initial_trajectory))\n",
    "\n",
    "        # Setup initial wsg trajectory\n",
    "        wsg_positions = np.zeros((1, 2))  # Single joint position, 2 time points\n",
    "        self.initial_wsg_trajectory = PiecewisePolynomial.FirstOrderHold(times, wsg_positions)\n",
    "        self.wsg_input = self.station.GetInputPort(\"wsg.position\")\n",
    "        self.dynamic_wsg_trajectory_source = self.builder.AddSystem(DynamicTrajectorySource(self.initial_wsg_trajectory))\n",
    "\n",
    "        self.ARM = SelfMadeArmDriver(\n",
    "            self.iiwa, \n",
    "            self.planner, \n",
    "            self.dynamic_trajectory_source, \n",
    "            self.dynamic_wsg_trajectory_source\n",
    "        )\n",
    "\n",
    "        self.builder.Connect(self.dynamic_trajectory_source.get_output_port(0), self.iiwa_position_input)\n",
    "        self.builder.Connect(self.dynamic_wsg_trajectory_source.get_output_port(0), self.wsg_input)\n",
    "\n",
    "        self.is_moving = False\n",
    "        self.end_move_time = None\n",
    "        self.cams = None\n",
    "\n",
    "    def set_perception_module(self, diagram, context):\n",
    "        self.perception = PerceptionModule(diagram, context)\n",
    "        self.diagram = diagram\n",
    "        self.context = context\n",
    "        self.predictions = self.perception.empty_mask()\n",
    "    def Update(self, context, event):\n",
    "        diagram = self.diagram\n",
    "        current_time = context.get_time()\n",
    "        #Update the cameras with the current time data\n",
    "        predicts = self.perception.empty_mask()\n",
    "\n",
    "        # if current_time == 12.0:\n",
    "        self.perception.update_cameras()\n",
    "            \n",
    "            #Run the masking model and get the predictions\n",
    "        predicts = self.perception.predict_mask(current_time)\n",
    "\n",
    "\n",
    "        #If not a valid prediction, don't do anything  \n",
    "        if (not self.perception.valid_prediction(predicts) and not self.perception.valid_prediction(self.predictions)):\n",
    "            # print(\"Not a valid prediction\")\n",
    "            curr_times_arr = np.array([current_time, current_time + 0.1])\n",
    "\n",
    "            current_position = self.iiwa.get_current_gripper_joint_position()\n",
    "            positions = np.tile(current_position, (2, 1)).T\n",
    "            curr_joint_traj = PiecewisePolynomial.FirstOrderHold(curr_times_arr, positions)\n",
    "\n",
    "            wsg_positions = np.zeros((1, 2))  # Single joint position, 2 time points\n",
    "            curr_wsg_traj = PiecewisePolynomial.FirstOrderHold(curr_times_arr, wsg_positions)\n",
    "\n",
    "        \n",
    "            update_trajectory(self.dynamic_trajectory_source, self.initial_trajectory)\n",
    "            update_trajectory(self.dynamic_wsg_trajectory_source, self.initial_wsg_trajectory)\n",
    "        else:\n",
    "            self.predictions = self.predictions if self.perception.valid_prediction(self.predictions) else predicts\n",
    "            print(\"Valid prediction\")\n",
    "\n",
    "            X_WG_curr = state_machine.iiwa.get_X_WG()\n",
    "            \n",
    "            #We obtain the antipodal grasps\n",
    "            \n",
    "\n",
    "\n",
    "            curr_times_arr = np.array([current_time, current_time + 0.1])\n",
    "\n",
    "            current_position = self.iiwa.get_current_gripper_joint_position()\n",
    "            positions = np.tile(current_position, (2, 1)).T\n",
    "            curr_joint_traj = PiecewisePolynomial.FirstOrderHold(curr_times_arr, positions)\n",
    "\n",
    "            wsg_positions = np.zeros((1, 2))  # Single joint position, 2 time points\n",
    "            curr_wsg_traj = PiecewisePolynomial.FirstOrderHold(curr_times_arr, wsg_positions)\n",
    "\n",
    "        \n",
    "            if current_time <= 7.0:\n",
    "                update_trajectory(self.dynamic_trajectory_source, self.initial_trajectory)\n",
    "                update_trajectory(self.dynamic_wsg_trajectory_source, self.initial_wsg_trajectory)\n",
    "            elif not self.is_moving:\n",
    "                self.is_moving = True\n",
    "                if not self.antipodal_calculated:\n",
    "                    translation, rotation = self.perception.get_antipodal_grasps(self.diagram, context, self.predictions, X_WG_curr)\n",
    "                    translation_1 = translation + np.array([0, 0, 0.2])\n",
    "                    translation_2 = translation + np.array([0, 0, 0.4])\n",
    "                \n",
    "                    follow_poses = [\n",
    "\n",
    "                        Desired_Pose(\n",
    "                            translation_1,\n",
    "                            rotation,\n",
    "                            1.0,\n",
    "                        ),\n",
    "                        Desired_Pose(\n",
    "                            translation,\n",
    "                            rotation,\n",
    "                            1.0,\n",
    "                        ),\n",
    "                        Desired_Pose(\n",
    "                            translation,\n",
    "                            rotation,\n",
    "                            0,\n",
    "                        ),\n",
    "                        Desired_Pose(\n",
    "                            translation_2,\n",
    "                            rotation,\n",
    "                            0,\n",
    "                        ),\n",
    "                        Desired_Pose(\n",
    "                            np.array([0.15, -0.8, 0.6]),\n",
    "                            rotation,\n",
    "                            0,\n",
    "                        ),\n",
    "                        Desired_Pose(\n",
    "                            np.array([0.15, -0.8, 0.6]),\n",
    "                            rotation,\n",
    "                            1.0,\n",
    "                        ),\n",
    "                    ]\n",
    "                    self.end_move_time = self.ARM.set_desired_poses_and_follow(follow_poses, current_time)\n",
    "                    self.antipodal_calculated = True\n",
    "            elif current_time >= self.end_move_time:\n",
    "                self.is_moving = False\n",
    "                update_trajectory(self.dynamic_trajectory_source, curr_joint_traj)\n",
    "                update_trajectory(self.dynamic_wsg_trajectory_source, curr_wsg_traj)\n",
    "        \n",
    "\n",
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cell_id": "74aa3dc93f2c4241bfd59c08443b33d2",
    "deepnote_cell_type": "code",
    "execution_context_id": "301fdf73-8afa-40db-a681-9791010114a3",
    "execution_millis": 0,
    "execution_start": 1732476147065,
    "source_hash": "9bddcad9"
   },
   "outputs": [],
   "source": [
    "from pydrake.systems.framework import DiagramBuilder, LeafSystem\n",
    "from pydrake.systems.primitives import ConstantVectorSource, Adder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Custom LeafSystem to apply torque at a specific joint index\n",
    "class TorqueMappingSystem(LeafSystem):\n",
    "    def __init__(self, joint_index, size, scenario_data):\n",
    "        super().__init__()\n",
    "        self.joint_index = joint_index\n",
    "        self.size = size\n",
    "        self.model_directive = scenario_data\n",
    "\n",
    "        # Declare the input and output ports\n",
    "        self.DeclareVectorInputPort(\"input_torque\", 1)  # Single torque value\n",
    "        self.DeclareVectorOutputPort(\n",
    "            \"output_torque_vector\",\n",
    "            size,\n",
    "            self.map_torque_to_vector\n",
    "        )\n",
    "\n",
    "    def map_torque_to_vector(self, context, output):\n",
    "        torque = self.get_input_port().Eval(context)[0]\n",
    "        torque_vector = np.zeros(self.size)\n",
    "        torque_vector[self.joint_index] = torque\n",
    "        # output[:] = torque_vector\n",
    "        \n",
    "        output.SetFromVector(torque_vector)\n",
    "\n",
    "\n",
    "def create_scene_with_motion(sim_time_step, model_directive, builder,station):\n",
    "    # Clean up the Meshcat instance.\n",
    "    meshcat.Delete()\n",
    "    meshcat.DeleteAddedControls()\n",
    "\n",
    "    # builder = DiagramBuilder()\n",
    "    # plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=sim_time_step)\n",
    "    # parser = Parser(plant)\n",
    "\n",
    "    # # Load the belt\n",
    "    # belt_model = parser.AddModels(belt_sdf_path)\n",
    "    # belt_frame = plant.GetFrameByName(\"disk_2_center\", belt_model[0])\n",
    "    # plant.WeldFrames(\n",
    "    #     plant.world_frame(),\n",
    "    #     belt_frame,\n",
    "    #     RigidTransform(RollPitchYaw(0, 0, 0), [0, 0, 0.25])\n",
    "    # )\n",
    "\n",
    "    # # Load the iiwa robot arm\n",
    "    # iiwa_model = parser.AddModels(url=\"package://drake_models/iiwa_description/sdf/iiwa7_with_box_collision.sdf\")\n",
    "    # iiwa_frame = plant.GetFrameByName(\"iiwa_link_0\", iiwa_model[0])\n",
    "    # plant.WeldFrames(\n",
    "    #     plant.world_frame(),\n",
    "    #     iiwa_frame,\n",
    "    #     RigidTransform(RollPitchYaw(0, 0, 0), [5.15, 0, 0.0])\n",
    "    # )\n",
    "\n",
    "    # # Add a rotating disk\n",
    "    # # disk_1_joint = AddRotatingDiskWithJoint(plant, \"disk_1\")\n",
    "    # # disk_1_joint_index = disk_1_joint.index()\n",
    "\n",
    "    # # Finalize the plant\n",
    "    # plant.Finalize()\n",
    "    \n",
    "    # scenario = LoadScenario(data= model_directive)\n",
    "    # # station = builder.AddSystem(MakeHardwareStation(\n",
    "    #     scenario, meshcat,hardware_station_name= \"belt_station\" , package_xmls=[os.getcwd() + \"/package.xml\"]\n",
    "    # ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Desired constant disk velocity\n",
    "    desired_velocity = 0.01  # Constant velocity in radians/second \n",
    "\n",
    "    # Create a ConstantVectorSource for velocity\n",
    "    velocity_source = builder.AddSystem(ConstantVectorSource([desired_velocity]))\n",
    "\n",
    "    # Create an Integrator to compute the position from velocity\n",
    "    position_integrator = builder.AddSystem(Integrator(1))  # Single-dimensional integrator\n",
    "\n",
    "    # Connect the velocity source to the integrator\n",
    "    builder.Connect(velocity_source.get_output_port(), position_integrator.get_input_port())\n",
    "\n",
    "    # Concatenate position and velocity into the desired_state vector\n",
    "    multiplexer = builder.AddSystem(Multiplexer(num_scalar_inputs=2))\n",
    "    builder.Connect(position_integrator.get_output_port(), multiplexer.get_input_port(0))  # Position\n",
    "    builder.Connect(velocity_source.get_output_port(), multiplexer.get_input_port(1))     # Velocity\n",
    "\n",
    "    # Connect the concatenated desired_state to your joint controller\n",
    "    # (Assume 'joint_controller' is already defined and added to the builder)\n",
    "    builder.Connect(multiplexer.get_output_port(), station.GetInputPort(\"dual_disk_model.desired_state\"))\n",
    "\n",
    "    # Visualization\n",
    "    # AddDefaultVisualization(builder=builder, meshcat=meshcat)\n",
    "\n",
    "    # Build the diagram\n",
    "    # diagram = builder.Build()\n",
    "    # return diagram\n",
    "\n",
    "\n",
    "# def run_simulation_with_motion(sim_time_step):\n",
    "#     diagram = create_scene_with_motion(sim_time_step)\n",
    "#     simulator = Simulator(diagram)\n",
    "#     simulator.set_target_realtime_rate(1.0)\n",
    "    \n",
    "#     context = simulator.get_mutable_context()\n",
    "#     # Fix constant position for the iiwa arm\n",
    "#     iiwa_position = [0, 0, 0, -1.57, 0, 1.57, 0]\n",
    "#     station = diagram.GetSubsystemByName(\"station\")\n",
    "#     station_context = station.GetMyMutableContextFromRoot(context)\n",
    "#     station.GetInputPort(\"iiwa.position\").FixValue(station_context, np.array(iiwa_position))\n",
    "    \n",
    "#     meshcat.StartRecording()\n",
    "#     simulator.AdvanceTo(120.0)  # Run simulation for 2 seconds\n",
    "#     meshcat.PublishRecording()\n",
    "\n",
    "\n",
    "# # Run the simulation with motion\n",
    "# run_simulation_with_motion(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "22ddef873e324dfb9efd556454c5367d",
    "deepnote_app_is_code_hidden": true,
    "deepnote_cell_type": "code",
    "execution_context_id": "301fdf73-8afa-40db-a681-9791010114a3",
    "execution_millis": 118944,
    "execution_start": 1732477045416,
    "is_code_hidden": false,
    "source_hash": "63fc84e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: MESA-LOADER: failed to open vgem: /usr/lib/dri/vgem_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: NEEDS EXTENSION: falling back to kms_swrast\n",
      "/root/64210_linux/64210-Project/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid prediction\n",
      "cloud past [[-0.6272575  -0.6637866  -0.6572961  ... -0.701811   -0.77585655\n",
      "  -0.77585644]\n",
      " [-0.25676328 -0.26521438 -0.26341397 ... -0.29227385 -0.30401105\n",
      "  -0.30624938]\n",
      " [ 0.30311847  0.30143756  0.30280265 ...  0.33748135  0.30048314\n",
      "   0.3004832 ]]\n",
      "cloud_xyz_future [[-0.61999947 -0.65595603 -0.6495852  ... -0.6922895  -0.7654979\n",
      "  -0.7653636 ]\n",
      " [ 0.01490051  0.00427419  0.00646056 ... -0.02501668 -0.04117282\n",
      "  -0.04340712]\n",
      " [ 0.30311847  0.30143756  0.30280265 ...  0.33748135  0.30048314\n",
      "   0.3004832 ]]\n",
      "RigidTransform(\n",
      "  R=RotationMatrix([\n",
      "    [-0.9871778972380233, 0.0061590842393893785, 0.15950506225837202],\n",
      "    [-0.15950183479684366, 0.0010148702657786224, -0.9871971103760304],\n",
      "    [-0.006242107108614947, -0.9999805176700574, -1.947225808984765e-05],\n",
      "  ]),\n",
      "  p=[-0.6381905890032036, -0.05747416159858456, 0.4240109470706069],\n",
      ")\n",
      "0.9908362353258816\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Success!\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n",
      "Valid prediction\n"
     ]
    }
   ],
   "source": [
    "from pydrake.all import (\n",
    "    DiagramBuilder,\n",
    "    Simulator,\n",
    "    ConstantValueSource,\n",
    "    Integrator,\n",
    "\n",
    ")\n",
    "\n",
    "# Assuming `LoadScenario`, `StateMachine`, `AddPointClouds`, `AddMultibodyTriad` are pre-defined\n",
    "scenario = LoadScenario(data=scenario_data)\n",
    "builder = DiagramBuilder()\n",
    "\n",
    "# State machine for handling scenarios\n",
    "state_machine = StateMachine(builder, meshcat)\n",
    "builder.AddSystem(state_machine)\n",
    "\n",
    "#Add belt motion\n",
    "create_scene_with_motion(0.1, scenario_data, builder, state_machine.station)\n",
    "\n",
    "# Export the camera outputs\n",
    "for idx in range(3):  # Assuming three cameras defined in directive\n",
    "    builder.ExportOutput(state_machine.station.GetOutputPort(f\"camera{idx}.rgb_image\"), f\"camera{idx}_rgb_image\")\n",
    "    builder.ExportOutput(state_machine.station.GetOutputPort(f\"camera{idx}.depth_image\"), f\"camera{idx}_depth_image\")\n",
    "\n",
    "# Add point clouds\n",
    "to_point_cloud = AddPointClouds(\n",
    "    scenario=scenario, station=state_machine.station, builder=builder, meshcat=meshcat\n",
    ")\n",
    "\n",
    "# Access plant and scene graph to add visual aids (optional)\n",
    "plant = state_machine.station.GetSubsystemByName(\"plant\")\n",
    "scene_graph = state_machine.station.GetSubsystemByName(\"scene_graph\")\n",
    "for idx in range(3):  # Assuming three cameras defined in directive\n",
    "    camera_instance = plant.GetModelInstanceByName(f\"camera{idx}_model\")\n",
    "    AddMultibodyTriad(\n",
    "        plant.GetFrameByName(\"base\", camera_instance),\n",
    "        scene_graph,\n",
    "        length=0.1,\n",
    "        radius=0.005,\n",
    "    )\n",
    "\n",
    "\n",
    "# Build the diagram and run the simulation\n",
    "diagram = builder.Build()\n",
    "simulator = Simulator(diagram)\n",
    "context = simulator.get_mutable_context()\n",
    "\n",
    "# Initialize the state machine's perception module\n",
    "state_machine.set_perception_module(diagram, context)\n",
    "\n",
    "# Publish initial state and start recording\n",
    "diagram.ForcedPublish(context)\n",
    "meshcat.StartRecording()\n",
    "\n",
    "# Advance simulation to 20 seconds\n",
    "simulator.AdvanceTo(30.0)\n",
    "meshcat.PublishRecording()\n",
    "\n",
    "# Uncomment if needed: Access and print the angular position at the end of the simulation\n",
    "# angular_position_context = angular_position_integrator.GetMyContextFromRoot(context)\n",
    "# print(\"Final Angular Position:\", angular_position_integrator.get_output_port().Eval(angular_position_context))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a778622e-2d6e-4ce6-abf4-4de46ff6a9a2' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "3d13e24574494f69a0f2976b3f440e3e",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
