{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"0d646be2ded440c9b3a80f64ec7f25a9","deepnote_cell_type":"markdown","id":"ej_CxPYTeFSo"},"source":["## **Movement and Static Setup**"]},{"cell_type":"code","metadata":{"source_hash":"d08a96c8","execution_start":1731976880176,"execution_millis":2833,"execution_context_id":"97d0f0bf-b734-4e05-aa2d-e0e6dba16378","cell_id":"76cb4432e1f4401cbfa70ae280f3ac4a","deepnote_cell_type":"code","id":"EfxtzA47eFS3","executionInfo":{"status":"error","timestamp":1732054150516,"user_tz":300,"elapsed":466,"user":{"displayName":"Amit","userId":"08720702918885979857"}},"outputId":"903fedec-08bd-4cf6-e201-695bee390970","colab":{"base_uri":"https://localhost:8080/","height":399}},"source":["import os\n","\n","import numpy as np\n","from pydrake.all import (\n","    AddDefaultVisualization,\n","    AddMultibodyPlantSceneGraph,\n","    DiagramBuilder,\n","    LoadModelDirectives,\n","    LoadModelDirectivesFromString,\n","    Parser,\n","    ProcessModelDirectives,\n","    RigidTransform,\n","    RollPitchYaw,\n","    Simulator,\n","    StartMeshcat,\n","    MeshcatVisualizer\n",")\n","from pydrake.common import temp_directory\n","from pydrake.geometry import StartMeshcat\n","from pydrake.systems.analysis import Simulator\n","from pydrake.visualization import ModelVisualizer\n","\n","\n","from manipulation.utils import RenderDiagram\n","from manipulation.meshcat_utils import AddMeshcatTriad\n","from manipulation import running_as_notebook\n","from manipulation.station import LoadScenario, MakeHardwareStation\n","from manipulation.utils import ConfigureParser\n","\n","from pydrake.all import (\n","    DiagramBuilder, Simulator, RigidTransform, RotationMatrix,\n","    InverseKinematics, Solve, MultibodyPlant, Parser, DirectCollocation, MathematicalProgram,\n",")\n","\n","from manipulation.meshcat_utils import AddMeshcatTriad\n","from pydrake.all import RotationMatrix, FixedOffsetFrame, MultibodyPlant, InverseKinematics, Solve, RotationMatrix\n","from pydrake.all import PiecewisePolynomial, TrajectorySource, ConstantVectorSource, Box, Rgba\n","from pydrake.symbolic import Formula\n","from pydrake.all import PiecewisePolynomial, TrajectorySource, ConstantVectorSource, Box, Rgba\n","import pydot\n","from scipy.spatial import KDTree\n","import random\n","import matplotlib.pyplot as plt\n","\n","import os\n","\n","import numpy as np\n","import tempfile\n","from copy import deepcopy\n","from urllib.request import urlretrieve\n","from pydrake.all import (\n","    AddDefaultVisualization,\n","    AddMultibodyPlantSceneGraph,\n","    DiagramBuilder,\n","    LoadModelDirectives,\n","    LoadModelDirectivesFromString,\n","    Parser,\n","    ProcessModelDirectives,\n","    RigidTransform,\n","    RollPitchYaw,\n","    Simulator,\n","    StartMeshcat,\n","    ProcessModelDirectives,\n","    LoadModelDirectives,\n","    RgbdSensor,\n","    RigidTransform,\n","    RollPitchYaw,\n","    ColorRenderCamera,\n","    DepthRenderCamera,\n","    MeshcatVisualizer,\n","    MeshcatVisualizerParams\n",")\n","from pydrake.common import temp_directory\n","from pydrake.geometry import StartMeshcat\n","from pydrake.systems.analysis import Simulator\n","from pydrake.visualization import ModelVisualizer\n","from manipulation import running_as_notebook\n","from manipulation.station import AddPointClouds, LoadScenario, MakeHardwareStation, Scenario\n","from manipulation.utils import ConfigureParser\n","from manipulation.scenarios import AddMultibodyTriad\n","from manipulation.meshcat_utils import AddMeshcatTriad\n","from pydrake.all import RotationMatrix, FixedOffsetFrame, MultibodyPlant, InverseKinematics, Solve, RotationMatrix\n","from pydrake.all import PiecewisePolynomial, TrajectorySource, ConstantVectorSource, Box, Rgba\n","from pydrake.symbolic import Formula\n","from pydrake.all import PiecewisePolynomial, TrajectorySource, ConstantVectorSource, Box, Rgba\n","import pydot\n","from scipy.spatial import KDTree\n","from scipy.stats import mode\n","import random\n","import matplotlib.pyplot as plt\n","from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","import os\n","from copy import deepcopy\n","from urllib.request import urlretrieve\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.utils.data\n","import torchvision\n","import torchvision.transforms.functional as Tf\n","from pydrake.all import (\n","    BaseField,\n","    Concatenate,\n","    Fields,\n","    MeshcatVisualizer,\n","    MeshcatVisualizerParams,\n","    PointCloud,\n","    StartMeshcat,\n",")\n","from pydrake.multibody.parsing import Parser\n","from pydrake.multibody.plant import AddMultibodyPlantSceneGraph\n","from pydrake.systems.framework import DiagramBuilder\n","from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","from manipulation import running_as_notebook\n","from manipulation.clutter import GenerateAntipodalGraspCandidate\n","from manipulation.scenarios import AddRgbdSensors\n","from manipulation.utils import ConfigureParser, FindDataResource\n","from manipulation.meshcat_utils import AddMeshcatTriad"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'pydrake'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-fb45eca937f9>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from pydrake.all import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mAddDefaultVisualization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mAddMultibodyPlantSceneGraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydrake'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"source_hash":"450eddfc","is_code_hidden":true,"execution_start":1731976883056,"execution_millis":0,"execution_context_id":"1e75b5fa-0da6-4637-becc-abc1c1697a10","deepnote_app_is_code_hidden":true,"cell_id":"f4f6847a5197489da2008b2ae1d1268b","deepnote_cell_type":"code","id":"WdfRLsbseFTF","executionInfo":{"status":"error","timestamp":1732054151253,"user_tz":300,"elapsed":39,"user":{"displayName":"Amit","userId":"08720702918885979857"}},"outputId":"c424b156-1338-41d2-ed5c-822a9ef473fb","colab":{"base_uri":"https://localhost:8080/","height":182}},"source":["# Start the visualizer. The cell will output an HTTP link after the execution.\n","# Click the link and a MeshCat tab should appear in your browser.\n","meshcat = StartMeshcat()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'StartMeshcat' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-22de182f6441>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start the visualizer. The cell will output an HTTP link after the execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Click the link and a MeshCat tab should appear in your browser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmeshcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStartMeshcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'StartMeshcat' is not defined"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"990d5631295f4d608c4f4e3ea0effb6c","deepnote_cell_type":"text-cell-h2","id":"ftpKYF7eeFTK"},"source":["## Load The Masking Model"]},{"cell_type":"code","metadata":{"source_hash":"b71b09c0","execution_start":1731976883108,"execution_millis":0,"execution_context_id":"1e75b5fa-0da6-4637-becc-abc1c1697a10","cell_id":"97b6130e45f040a1829ada17e8318f8e","deepnote_cell_type":"code","id":"z-Nfvhh_eFTL"},"source":["if running_as_notebook:\n","    model_file = \"clutter_maskrcnn_model.pt\"\n","    if not os.path.exists(model_file):\n","        urlretrieve(\n","            \"https://groups.csail.mit.edu/locomotion/clutter_maskrcnn_model.pt\",\n","            model_file,\n","        )\n","# ycb = [\n","#     \"003_cracker_box.sdf\",\n","#     \"004_sugar_box.sdf\",\n","#     \"005_tomato_soup_can.sdf\",\n","#     \"006_mustard_bottle.sdf\",\n","#     \"009_gelatin_box.sdf\",\n","#     \"010_potted_meat_can.sdf\",\n","# ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"c0084ad2","is_code_hidden":true,"execution_start":1731976883164,"execution_millis":918,"execution_context_id":"97d0f0bf-b734-4e05-aa2d-e0e6dba16378","deepnote_app_is_code_hidden":true,"cell_id":"26720beda6b541439b1cea01f33d53df","deepnote_cell_type":"code","id":"Grvb219reFTN"},"source":["if running_as_notebook:\n","\n","    def get_instance_segmentation_model(num_classes):\n","        # load an instance segmentation model pre-trained on COCO\n","        model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n","            weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT, progress=False\n","        )\n","\n","        # get the number of input features for the classifier\n","        in_features = model.roi_heads.box_predictor.cls_score.in_features\n","        # replace the pre-trained head with a new one\n","        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","        # now get the number of input features for the mask classifier\n","        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","        hidden_layer = 256\n","        # and replace the mask predictor with a new one\n","        model.roi_heads.mask_predictor = MaskRCNNPredictor(\n","            in_features_mask, hidden_layer, num_classes\n","        )\n","\n","        return model\n","\n","    num_classes = 7\n","    model = get_instance_segmentation_model(num_classes)\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    model.load_state_dict(torch.load(\"clutter_maskrcnn_model.pt\", map_location=device))\n","    model.eval()\n","\n","    model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":30,"fromCodePoint":8}],"cell_id":"a7ed7d3c38da468c8a63f9323dae1dc7","deepnote_cell_type":"text-cell-p","id":"6B3SxWRpeFTO"},"source":["Use the get_merged_masked_pcd function below to get the voxelized point cloud configuration from prediction data via the model and the image data from the cameras"]},{"cell_type":"code","metadata":{"source_hash":"48d64d71","execution_start":1731976884136,"execution_millis":0,"execution_context_id":"1e75b5fa-0da6-4637-becc-abc1c1697a10","cell_id":"8ed08215d3db4cd19694d5607ffd452e","deepnote_cell_type":"code","id":"siF9towfeFTQ"},"source":["def get_merged_masked_pcd(\n","   predictions,\n","   rgb_ims,\n","   depth_ims,\n","   project_depth_to_pC_funcs,\n","   X_WCs,\n","   label,\n","   mask_threshold=150,\n","):\n","   \"\"\"\n","   predictions: The output of the trained network (one for each camera)\n","   rgb_ims: RGBA images from each camera\n","   depth_ims: Depth images from each camera\n","   project_depth_to_pC_funcs: Functions that perform the pinhole camera operations to convert pixels\n","       into points. See the analogous function in problem 5.2 to see how to use it.\n","   X_WCs: Poses of the cameras in the world frame\n","   mask_threshold: Use this to determine which pixels in\n","   \"\"\"\n","\n","   pcd = []\n","   for prediction, rgb_im, depth_im, project_depth_to_pC_func, X_WC in zip(\n","       predictions, rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs\n","   ):\n","       # These arrays aren't the same size as the correct outputs, but we're\n","       # just initializing them to something valid for now.\n","       spatial_points = np.zeros((3, 1))  # 3xN: (x,y,z) x Number of masked points\n","       rgb_points = np.zeros((3, 1))  # 3xN: Color channels x Number of masked points\n","\n","       ######################################\n","       # Your code here (populate spatial_points and rgb_points)\n","\n","       mask_idx = np.argmax(prediction[0][\"labels\"] == label)\n","       mask = prediction[0][\"masks\"][mask_idx, 0]\n","       idxs = mask > mask_threshold\n","\n","       # Mask pixels, then get points in camera frame\n","       u_range = np.arange(depth_im.shape[0])\n","       v_range = np.arange(depth_im.shape[1])\n","       depth_v, depth_u = np.meshgrid(v_range, u_range)\n","       depth_pnts = np.dstack([depth_u, depth_v, depth_im])\n","       masked_depth_pnts = depth_pnts[idxs]\n","       pC = np.expand_dims(project_depth_to_pC_func(masked_depth_pnts), 2)\n","\n","       # Convert to world frame\n","       R = np.expand_dims(X_WC.rotation().matrix(), 0)\n","       p = np.expand_dims(X_WC.translation(), [0, 2])\n","       pC_in_world = np.matmul(R, pC) + p\n","       spatial_points = np.squeeze(pC_in_world).T\n","\n","       color_pnts = rgb_im\n","       color_pnts = color_pnts[idxs]\n","       rgb_points = color_pnts[:, :3].T\n","\n","       ######################################\n","\n","       # You get an unhelpful RunTime error if your arrays are the wrong\n","       # shape, so we'll check beforehand that they're the correct shapes.\n","       assert (\n","           len(spatial_points.shape) == 2\n","       ), \"Spatial points is the wrong size -- should be 3 x N\"\n","       assert (\n","           spatial_points.shape[0] == 3\n","       ), \"Spatial points is the wrong size -- should be 3 x N\"\n","       assert (\n","           len(rgb_points.shape) == 2\n","       ), \"RGB points is the wrong size -- should be 3 x N\"\n","       assert (\n","           rgb_points.shape[0] == 3\n","       ), \"RGB points is the wrong size -- should be 3 x N\"\n","       assert rgb_points.shape[1] == spatial_points.shape[1]\n","\n","       N = spatial_points.shape[1]\n","       pcd.append(PointCloud(N, Fields(BaseField.kXYZs | BaseField.kRGBs)))\n","       pcd[-1].mutable_xyzs()[:] = spatial_points\n","       pcd[-1].mutable_rgbs()[:] = rgb_points\n","       # Estimate normals\n","       pcd[-1].EstimateNormals(radius=0.1, num_closest=30)\n","       # Flip normals toward camera\n","       pcd[-1].FlipNormalsTowardPoint(X_WC.translation())\n","\n","   # Merge point clouds.\n","   merged_pcd = Concatenate(pcd)\n","\n","   # Voxelize down-sample.  (Note that the normals still look reasonable)\n","   return merged_pcd.VoxelizedDownSample(voxel_size=0.005)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"29ef0f2b","execution_start":1731976884188,"execution_millis":0,"execution_context_id":"4a5ede74-7bd7-4664-887b-35842c458f57","cell_id":"9a884dd946224f6fb8feabb49d40df93","deepnote_cell_type":"code","id":"RblF8MQseFTS"},"source":["# def GenerateAntipodalGraspCandidate(\n","#     diagram,\n","#     context,\n","#     cloud,\n","#     rng,\n","#     wsg_body_index=None,\n","#     scene_graph_system_name=\"scene_graph\",\n","# ):\n","#     \"\"\"\n","#     Picks a random point in the cloud, and aligns the robot finger with the\n","#     normal of that pixel. The rotation around the normal axis is drawn from a\n","#     uniform distribution over [min_roll, max_roll].\n","\n","#     Args:\n","#         diagram: A diagram containing a MultibodyPlant+SceneGraph that contains\n","#             a free body gripper and any obstacles in the environment that we\n","#             want to check collisions against. It should not include the objects\n","#             in the point cloud; those are handled separately.\n","#         context: The diagram context.  All positions in the context will be\n","#             held fixed *except* the gripper free body pose.\n","#         cloud: a PointCloud in world coordinates which represents candidate\n","#             grasps.\n","#         rng: a np.random.default_rng()\n","#         wsg_body_index: The body index of the gripper in plant.  If None,\n","#             then a body named \"body\" will be searched for in the plant.\n","\n","#     Returns:\n","#         cost: The grasp cost X_G: The grasp candidate\n","#     \"\"\"\n","#     station = diagram.GetSubsystemByName(\"station\")\n","#     plant = station.GetSubsystemByName(\"plant\")\n","#     plant_context = plant.GetMyMutableContextFromRoot(station.GetMyContextFromRoot(context))\n","#     scene_graph = station.GetSubsystemByName(scene_graph_system_name)\n","#     scene_graph.GetMyMutableContextFromRoot(station.GetMyContextFromRoot(context))\n","#     if wsg_body_index:\n","#         wsg = plant.get_body(wsg_body_index)\n","#     else:\n","#         wsg = plant.GetBodyByName(\"body\")\n","#         wsg_body_index = wsg.index()\n","\n","#     if cloud.size() < 1:\n","#         return np.inf, None\n","\n","#     index = rng.integers(0, cloud.size() - 1)\n","\n","#     # Use S for sample point/frame.\n","#     p_WS = cloud.xyz(index)\n","#     n_WS = cloud.normal(index)\n","\n","#     assert np.isclose(\n","#         np.linalg.norm(n_WS), 1.0\n","#     ), f\"Normal has magnitude: {np.linalg.norm(n_WS)}\"\n","\n","#     Gx = n_WS  # gripper x axis aligns with normal\n","#     # make orthonormal y axis, aligned with world down\n","#     y = np.array([0.0, 0.0, -1.0])\n","#     if np.abs(np.dot(y, Gx)) < 1e-6:\n","#         # normal was pointing straight down.  reject this sample.\n","#         return np.inf, None\n","\n","#     Gy = y - np.dot(y, Gx) * Gx\n","#     Gz = np.cross(Gx, Gy)\n","#     R_WG = RotationMatrix(np.vstack((Gx, Gy, Gz)).T)\n","#     p_GS_G = [0.054 - 0.01, 0.10625, 0]\n","\n","#     # Try orientations from the center out\n","#     min_roll = -np.pi / 3.0\n","#     max_roll = np.pi / 3.0\n","#     alpha = np.array([0.5, 0.65, 0.35, 0.8, 0.2, 1.0, 0.0])\n","#     for theta in min_roll + (max_roll - min_roll) * alpha:\n","#         # Rotate the object in the hand by a random rotation (around the\n","#         # normal).\n","#         R_WG2 = R_WG.multiply(RotationMatrix.MakeXRotation(theta))\n","\n","#         # Use G for gripper frame.\n","#         p_SG_W = -R_WG2.multiply(p_GS_G)\n","#         p_WG = p_WS + p_SG_W\n","\n","#         X_G = RigidTransform(R_WG2, p_WG)\n","#         plant.SetFreeBodyPose(plant_context, wsg, X_G)\n","#         cost = GraspCandidateCost(diagram, context, cloud, adjust_X_G=True)\n","#         X_G = plant.GetFreeBodyPose(plant_context, wsg)\n","#         if np.isfinite(cost):\n","#             return cost, X_G\n","\n","#     return np.inf, None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":29,"fromCodePoint":8}],"cell_id":"aa8468fbe1f943559e0667746c26417f","deepnote_cell_type":"text-cell-p","id":"46_gQc-ceFTU"},"source":["Use the find_antipodal_grasp function below to get the optimal gripper position based on the isolated point cloud of the object of interest. This function assumes no obstacles."]},{"cell_type":"code","metadata":{"source_hash":"2bef07df","execution_start":1731976884240,"execution_millis":0,"execution_context_id":"6a525ce4-f6f0-4e45-a08b-55a6f19c5564","cell_id":"031ff41db50c4a7f86215b680c2774c7","deepnote_cell_type":"code","id":"V8cxq6wFeFTV"},"source":["def find_antipodal_grasp(environment_diagram, environment_context, cameras, label, predictions):\n","    rng = np.random.default_rng()\n","\n","    # Another diagram for the objects the robot \"knows about\": gripper, cameras, bins.  Think of this as the model in the robot's head.\n","    builder = DiagramBuilder()\n","    plant, scene_graph = AddMultibodyPlantSceneGraph(builder, time_step=0.001)\n","    parser = Parser(plant)\n","    ConfigureParser(parser)\n","    parser.AddModelsFromUrl(\n","        \"package://manipulation/schunk_wsg_50_welded_fingers.dmd.yaml\"\n","    )\n","    plant.Finalize()\n","\n","    params = MeshcatVisualizerParams()\n","    params.prefix = \"planning\"\n","    visualizer = MeshcatVisualizer.AddToBuilder(builder, scene_graph, meshcat, params)\n","    diagram = builder.Build()\n","    context = diagram.CreateDefaultContext()\n","    diagram.ForcedPublish(context)\n","\n","    for c in cameras:\n","        c.compute_camera_data()\n","    rgb_ims = [c.rgb_im for c in cameras]\n","    depth_ims = [c.depth_im for c in cameras]\n","    project_depth_to_pC_funcs = [c.project_depth_to_pC for c in cameras]\n","    X_WCs = [c.X_WC for c in cameras]\n","\n","    cloud = get_merged_masked_pcd(\n","        predictions, rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs, label\n","    )\n","\n","    plant_context = plant.GetMyContextFromRoot(context)\n","    scene_graph_context = scene_graph.GetMyContextFromRoot(context)\n","\n","    min_cost = np.inf\n","    best_X_G = RigidTransform()\n","    for i in range(100):\n","        cost, X_G = GenerateAntipodalGraspCandidate(diagram, context, cloud, rng)\n","        if np.isfinite(cost) and cost < min_cost:\n","            min_cost = cost\n","            best_X_G = X_G\n","\n","    plant.SetFreeBodyPose(plant_context, plant.GetBodyByName(\"body\"), best_X_G)\n","    diagram.ForcedPublish(context)\n","\n","    return best_X_G\n","\n","\n","# if running_as_notebook:\n","#     find_antipodal_grasp(environment_diagram, environment_context, cameras)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"6886b870","is_code_hidden":false,"execution_start":1731976884296,"execution_millis":0,"execution_context_id":"4a5ede74-7bd7-4664-887b-35842c458f57","deepnote_app_is_code_hidden":true,"cell_id":"9961b64ed961454bac916e26eb2d9d08","deepnote_cell_type":"code","id":"yBDrY7WBeFTW"},"source":["class CameraSystem:\n","    def __init__(self, idx, meshcat, diagram, context):\n","        self.idx = idx\n","        self.context = context\n","        self.diagram = diagram\n","\n","        # # Read images\n","        # depth_im_read = (\n","        #     diagram.GetOutputPort(\"camera{}_depth_image\".format(idx))\n","        #     .Eval(context)\n","        #     .data.squeeze()\n","        # )\n","        # self.depth_im = deepcopy(depth_im_read)\n","        # self.depth_im[self.depth_im == np.inf] = 10.0\n","        # self.rgb_im = (\n","        #     diagram.GetOutputPort(\"camera{}_rgb_image\".format(idx)).Eval(context).data\n","        # )\n","\n","        # # Get other info about the camera\n","        # # cam = diagram.GetSubsystemByName(\"camera\" + str(idx))\n","        # # cam_context = cam.GetMyMutableContextFromRoot(context)\n","        # pose_subsystem = diagram.GetSubsystemByName(f\"camera{idx}.pose\")\n","        # pose_subsystem_context = pose_subsystem.GetMyContextFromRoot(context)  # Get specific context for ExtractPose subsystem\n","        # pose_output = pose_subsystem.GetOutputPort(\"pose\")\n","        # pose_data = pose_output.Eval(pose_subsystem_context)\n","        # #camera_frame = plant.GetFrameByName(\"base\", plant.GetModelInstanceByName(f\"camera{idx}_model\"))\n","        # cam = diagram.GetSubsystemByName(\"station\").GetSubsystemByName(\"rgbd_sensor_camera\" + str(idx))\n","        # self.X_WC = pose_data\n","        # self.cam_info = cam.default_depth_render_camera().core().intrinsics()\n","        # # self.fx = 525  # Focal length in pixels\n","        # self.fy = 525  # Focal length in pixels\n","        # self.cx = 320  # Principal point x-coordinate for a 640x480 image\n","        # self.cy = 240  # Principal point y-coordinate for a 640x480 image\n","\n","\n","\n","\n","    def project_depth_to_pC(self, depth_pixel):\n","        \"\"\"\n","        project depth pixels to points in camera frame\n","        using pinhole camera model\n","        Input:\n","            depth_pixels: numpy array of (nx3) or (3,)\n","        Output:\n","            pC: 3D point in camera frame, numpy array of (nx3)\n","        \"\"\"\n","        # switch u,v due to python convention\n","        v = depth_pixel[:, 0]\n","        u = depth_pixel[:, 1]\n","        Z = depth_pixel[:, 2]\n","        cx = self.cam_info.center_x()\n","        cy = self.cam_info.center_y()\n","        fx = self.cam_info.focal_x()\n","        fy = self.cam_info.focal_y()\n","        X = (u - cx) * Z / fx\n","        Y = (v - cy) * Z / fy\n","        pC = np.c_[X, Y, Z]\n","        return pC\n","\n","    def compute_camera_data(self):\n","        # Read images\n","        context = self.context\n","        diagram = self.diagram\n","        depth_im_read = (\n","            diagram.GetOutputPort(\"camera{}_depth_image\".format(self.idx))\n","            .Eval(context)\n","            .data.squeeze()\n","        )\n","        self.depth_im = deepcopy(depth_im_read)\n","        self.depth_im[self.depth_im == np.inf] = 10.0\n","        self.rgb_im = (\n","            diagram.GetOutputPort(\"camera{}_rgb_image\".format(self.idx)).Eval(context).data\n","        )\n","\n","        # Get other info about the camera\n","        # cam = diagram.GetSubsystemByName(\"camera\" + str(idx))\n","        # cam_context = cam.GetMyMutableContextFromRoot(context)\n","        pose_subsystem = diagram.GetSubsystemByName(f\"camera{self.idx}.pose\")\n","        pose_subsystem_context = pose_subsystem.GetMyContextFromRoot(context)  # Get specific context for ExtractPose subsystem\n","        pose_output = pose_subsystem.GetOutputPort(\"pose\")\n","        pose_data = pose_output.Eval(pose_subsystem_context)\n","        #camera_frame = plant.GetFrameByName(\"base\", plant.GetModelInstanceByName(f\"camera{idx}_model\"))\n","        cam = diagram.GetSubsystemByName(\"station\").GetSubsystemByName(\"rgbd_sensor_camera\" + str(self.idx))\n","        self.X_WC = pose_data\n","        self.cam_info = cam.default_depth_render_camera().core().intrinsics()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"2fb9247a","is_code_hidden":false,"execution_start":1731976884348,"execution_millis":0,"execution_context_id":"4a5ede74-7bd7-4664-887b-35842c458f57","deepnote_app_is_code_hidden":true,"cell_id":"1d4b35f1aa3a4faa8ef2ad333b914cbb","deepnote_cell_type":"code","id":"WqFwzaZLeFTY"},"source":["# - add_model:\n","#     name: table_top\n","#     file: package://dummy_project/table_top.sdf\n","\n","# - add_weld:\n","#     parent: world\n","#     child: table_top::table_top_center\n","\n","# - add_model:\n","#         name: dual_disk_model\n","#         file: package://dummy_project/belt.sdf\n","#         default_free_body_pose:\n","#             disk_2:\n","#                 translation: [-5.15, 0, 0.8]\n","#                 rotation: !Rpy { deg: [0, 0, 0] }\n","\n","# - add_weld:\n","#     parent: table_top::table_top_center\n","#     child: dual_disk_model::disk_2_center\n","#     X_PC:\n","#         rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n","#         translation: [-5.15, 0, 0.25]\n","\n","# - add_model:\n","#         name: bin_model\n","#         file: package://dummy_project/recyclable_bin.sdf\n","#         default_free_body_pose:\n","#             bin_base:\n","#                 translation: [-4.65, -0.5, 0.8]\n","#                 rotation: !Rpy { deg: [0, 0, 0] }\n","\n","# - add_weld:\n","#     parent: table_top::table_top_center\n","#     child: bin_model::bin_front_top_center\n","#     X_PC:\n","#         rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n","#         translation: [0.35, 0.8, 0.25]\n","\n","# - add_model:\n","#     name: non_bin_model\n","#     file: package://dummy_project/non_recyclable_bin.sdf\n","#     default_free_body_pose:\n","#         bin_base:\n","#             translation: [-5.65, 0.5, 0.8]\n","#             rotation: !Rpy { deg: [0, 0, 0] }\n","\n","# - add_weld:\n","#     parent: table_top::table_top_center\n","#     child: non_bin_model::bin_front_top_center\n","#     X_PC:\n","#         rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n","#         translation: [0.35, -0.8, 0.25]\n","\n","# - add_model:\n","#     name: cube_model\n","#     file: package://drake_models/ycb/004_sugar_box.sdf\n","#     default_free_body_pose:\n","#         base_link_sugar:\n","#             translation: [-0.45, 0, 0.8]\n","#             rotation: !Rpy { deg: [0, 0, 0] }\n","\n","scenario_data = \"\"\"\n","directives:\n","- add_model:\n","    name: table_top\n","    file: package://dummy_project/table_top.sdf\n","\n","- add_weld:\n","    parent: world\n","    child: table_top::table_top_center\n","\n","- add_model:\n","    name: dual_disk_model\n","    file: package://dummy_project/belt.sdf\n","    default_free_body_pose:\n","        disk_2:\n","            translation: [-5.15, 0, 0.8]\n","            rotation: !Rpy { deg: [0, 0, 0] }\n","\n","- add_weld:\n","    parent: table_top::table_top_center\n","    child: dual_disk_model::disk_2_center\n","    X_PC:\n","        rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n","        translation: [-5.15, 0, 0.25]\n","\n","- add_model:\n","    name: bin_model\n","    file: package://dummy_project/recyclable_bin.sdf\n","    default_free_body_pose:\n","        bin_base:\n","            translation: [-4.65, -0.5, 0.8]\n","            rotation: !Rpy { deg: [0, 0, 0] }\n","\n","- add_weld:\n","    parent: table_top::table_top_center\n","    child: bin_model::bin_front_top_center\n","    X_PC:\n","        rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n","        translation: [0.35, 0.8, 0.25]\n","\n","- add_model:\n","    name: non_bin_model\n","    file: package://dummy_project/non_recyclable_bin.sdf\n","    default_free_body_pose:\n","        bin_base:\n","            translation: [-5.65, 0.5, 0.8]\n","            rotation: !Rpy { deg: [0, 0, 0] }\n","\n","- add_weld:\n","    parent: table_top::table_top_center\n","    child: non_bin_model::bin_front_top_center\n","    X_PC:\n","        rotation: !Rpy { deg: [0.0, 0.0, 0.0] }\n","        translation: [0.35, -0.8, 0.25]\n","\n","- add_model:\n","    name: cube_model\n","    file: package://drake_models/ycb/004_sugar_box.sdf\n","    default_free_body_pose:\n","        base_link_sugar:\n","            translation: [-0.55, 0, 0.8]\n","            rotation: !Rpy { deg: [0, 0, 0] }\n","\n","- add_model:\n","    name: iiwa\n","    file: package://drake_models/iiwa_description/sdf/iiwa7_no_collision.sdf\n","    default_joint_positions:\n","        iiwa_joint_1: [-1.57]\n","        iiwa_joint_2: [0.1]\n","        iiwa_joint_3: [0]\n","        iiwa_joint_4: [-1.2]\n","        iiwa_joint_5: [0]\n","        iiwa_joint_6: [1.6]\n","        iiwa_joint_7: [0]\n","\n","- add_weld:\n","    parent: world\n","    child: iiwa::iiwa_link_0\n","    X_PC:\n","        translation: [0, 0, 0]\n","        rotation: !Rpy { deg: [0, 0, -90] }\n","\n","- add_model:\n","    name: wsg\n","    file: package://drake_models/wsg_50_description/sdf/schunk_wsg_50_with_tip.sdf\n","\n","- add_weld:\n","    parent: iiwa::iiwa_link_7\n","    child: wsg::body\n","    X_PC:\n","        translation: [0, 0, 0.09]\n","        rotation: !Rpy { deg: [90, 0, 90] }\n","\n","- add_frame:\n","    name: cameras_frame\n","    X_PF:\n","        base_frame: world\n","        translation: [-0.55, 0, 0.4]\n","- add_frame:\n","    name: camera0_frame\n","    X_PF:\n","        base_frame: cameras_frame\n","        translation: [0.4, 0, 0.2]\n","- add_frame:\n","    name: camera1_frame\n","    X_PF:\n","        base_frame: cameras_frame\n","        translation: [-0.4, 0, 0.2]\n","\n","- add_frame:\n","    name: camera2_frame\n","    X_PF:\n","        base_frame: cameras_frame\n","        translation: [0, 0.4, 0.2]\n","\n","\n","- add_model:\n","    name: camera0_model\n","    file: package://manipulation/camera_box.sdf\n","\n","- add_weld:\n","    parent: camera0_frame\n","    child: camera0_model::base\n","    X_PC:\n","        rotation: !Rpy { deg: [-120, 0, 90] }\n","\n","- add_model:\n","    name: camera1_model\n","    file: package://manipulation/camera_box.sdf\n","\n","- add_weld:\n","    parent: camera1_frame\n","    child: camera1_model::base\n","    X_PC:\n","        rotation: !Rpy { deg: [-120, 0, -90] }\n","\n","- add_model:\n","    name: camera2_model\n","    file: package://manipulation/camera_box.sdf\n","\n","- add_weld:\n","    parent: camera2_frame\n","    child: camera2_model::base\n","    X_PC:\n","        rotation: !Rpy { deg: [-120, 0, 180] }\n","plant_config:\n","    time_step: 1e-2\n","    contact_model: \"hydroelastic_with_fallback\"\n","    discrete_contact_approximation: \"sap\"\n","\n","\n","model_drivers:\n","    iiwa: !IiwaDriver\n","        control_mode: position_only\n","        hand_model_name: wsg\n","    wsg: !SchunkWsgDriver {}\n","cameras:\n","    main_camera:\n","        name: camera0\n","        depth: True\n","        X_PB:\n","            base_frame: camera0_model::base\n","    secondary_camera:\n","        name: camera1\n","        depth: True\n","        X_PB:\n","            base_frame: camera1_model::base\n","    third_camera:\n","        name: camera2\n","        depth: True\n","        X_PB:\n","            base_frame: camera2_model::base\n","\n","\"\"\"\n","\n","class IIWA_ARM:\n","    def __init__(self, builder, scenario, meshcat):\n","        self.builder = builder\n","        self.meshcat = meshcat\n","        self.station = self.builder.AddSystem(\n","            MakeHardwareStation(\n","                scenario,\n","                self.meshcat,\n","                package_xmls=[os.getcwd() + \"/package.xml\"])\n","        )\n","        self.plant = self.station.GetSubsystemByName(\"plant\")\n","\n","        self.max_tries = 10\n","\n","        self.iiwa_model_instance = self.plant.GetModelInstanceByName(\"iiwa\")\n","\n","        self.gripper_frame = self.plant.GetFrameByName(\"body\")\n","        self.world_frame = self.plant.world_frame()\n","\n","        self.visualizer = MeshcatVisualizer.AddToBuilder(\n","            self.builder, self.station.GetOutputPort(\"query_object\"), self.meshcat\n","        )\n","\n","        self.context = self.station.CreateDefaultContext()\n","        self.station_context = self.station.GetMyMutableContextFromRoot(self.context)\n","        self.plant_context = self.plant.GetMyMutableContextFromRoot(self.context)\n","\n","    def get_station(self):\n","        return self.station\n","\n","    def get_plant(self):\n","        return self.plant\n","\n","    def num_input_ports(self):\n","        return self.station.num_input_ports()\n","\n","    def visualize_frame(self, name, X_WF, length=0.15, radius=0.006):\n","        AddMeshcatTriad(\n","            self.meshcat, name, length=length, radius=radius, X_PT=X_WF\n","        )\n","\n","    def visualize_gripper_at_target(self, target_joint_positions):\n","        context = self.plant.CreateDefaultContext()\n","        self.plant.SetPositions(context, self.iiwa_model_instance, target_joint_positions)\n","\n","        X_WG_target = self.plant.CalcRelativeTransform(\n","            context, frame_A=self.world_frame, frame_B=self.gripper_frame\n","        )\n","        self.visualize_frame(\"gripper_target_pose\", X_WG_target)\n","\n","    def clear_all_triads(self):\n","        self.meshcat.Delete()\n","\n","    def get_current_gripper_joint_position(self):\n","        return self.plant.GetPositions(self.plant_context, self.plant.GetModelInstanceByName(\"iiwa\"))\n","\n","    def get_X_WG(self):\n","        return self.plant.CalcRelativeTransform(\n","            self.plant_context, self.world_frame, self.gripper_frame\n","        )\n","\n","    def get_X_WTarget(self, translation):\n","        X_WG = self.get_X_WG()\n","        rotation_matrix = RotationMatrix.Identity()\n","        X_GTarget = RigidTransform(\n","            rotation_matrix,\n","            translation\n","        )\n","\n","        X_WTarget = X_WG @ X_GTarget\n","        return X_WTarget\n","\n","    def find_joint_positions(self, X_WTarget):\n","        context = self.plant.CreateDefaultContext()\n","        end_effector_frame = self.plant.GetFrameByName('body')\n","\n","        ik = InverseKinematics(self.plant, context)\n","        ik.AddPositionConstraint(\n","            frameB=end_effector_frame,\n","            p_BQ=np.zeros((3, 1)),\n","            frameA=self.plant.world_frame(),\n","            p_AQ_lower=X_WTarget.translation().reshape((3, 1)) - 0.01,\n","            p_AQ_upper=X_WTarget.translation().reshape((3, 1)) + 0.01\n","        )\n","\n","        last_joint_positions = None\n","        for _ in range (self.max_tries):\n","            q0 = self.plant.GetPositions(context).copy()\n","            ik.prog().SetInitialGuess(ik.q(), q0)\n","\n","            # Solve the inverse kinematics problem\n","            result = Solve(ik.prog())\n","            # print(result.GetSolution(ik.q()[1:8]))\n","            # last_joint_positions = result.GetSolution(ik.q()[:7])\n","            last_joint_positions = result.GetSolution(ik.q()[1:8])\n","            if result.is_success():\n","                print(\"Success!\")\n","                joint_positions = last_joint_positions\n","                return joint_positions\n","\n","        return last_joint_positions\n","\n","        # # Solve the inverse kinematics problem\n","        # result = Solve(ik.prog())\n","        # joint_positions = result.GetSolution(ik.q()[:7])\n","        # return joint_positions\n","\n","    def move_arm(self, trajectory_source, wsg_src = None):\n","        self.builder.AddSystem(trajectory_source)\n","\n","        iiwa_position_input = self.station.GetInputPort(\"iiwa.position\")\n","        self.builder.Connect(trajectory_source.get_output_port(), iiwa_position_input)\n","\n","        wsg_input = self.station.GetInputPort(\"wsg.position\")\n","\n","        if wsg_src is None:\n","            wsg_src = ConstantVectorSource(np.zeros(wsg_input.size()))\n","\n","        self.builder.AddSystem(wsg_src)\n","        self.builder.Connect(wsg_src.get_output_port(), wsg_input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"28ee8414","is_code_hidden":false,"execution_start":1731976884404,"execution_millis":0,"execution_context_id":"4a5ede74-7bd7-4664-887b-35842c458f57","deepnote_app_is_code_hidden":true,"cell_id":"2f76fb5d4e4742a083aea2bb720cb8a5","deepnote_cell_type":"code","id":"kM710DtZeFTb"},"source":["class TrajOptPlanner:\n","    def __init__(self, num_time_samples=20):\n","        self.num_time_samples = num_time_samples\n","\n","    def plan(self, start, goal):\n","        prog = MathematicalProgram()\n","        n = len(start)\n","        h = 1.0 / (self.num_time_samples - 1)\n","\n","        # Decision variables for joint positions and velocities\n","        q = prog.NewContinuousVariables(self.num_time_samples, n, \"q\")\n","        v = prog.NewContinuousVariables(self.num_time_samples, n, \"v\")\n","\n","        # Initial and final state constraints\n","        prog.AddBoundingBoxConstraint(start, start, q[0])\n","        prog.AddBoundingBoxConstraint(goal, goal, q[-1])\n","        prog.AddBoundingBoxConstraint(np.zeros(n), np.zeros(n), v[0])\n","        prog.AddBoundingBoxConstraint(np.zeros(n), np.zeros(n), v[-1])\n","\n","        for i in range(self.num_time_samples - 1):\n","            q_next = q[i] + h * v[i]\n","            for j in range(n):\n","                prog.AddLinearEqualityConstraint(q_next[j] == q[i + 1][j])\n","\n","        R = 10  # Cost on input \"effort\".\n","        for i in range(self.num_time_samples - 1):\n","            u = (v[i + 1] - v[i]) / h\n","            prog.AddQuadraticCost(R * u.dot(u))\n","\n","        result = Solve(prog)\n","        if not result.is_success():\n","            raise RuntimeError(\"Trajectory optimization failed\")\n","\n","        q_trajectory = result.GetSolution(q)\n","        v_trajectory = result.GetSolution(v)\n","        times = np.linspace(0, 4, self.num_time_samples)\n","\n","        return times, q_trajectory, v_trajectory\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"cb2e33bd","is_code_hidden":false,"execution_start":1731976884452,"execution_millis":0,"execution_context_id":"4a5ede74-7bd7-4664-887b-35842c458f57","deepnote_app_is_code_hidden":true,"cell_id":"9528a3ce74264cf59a397cfa9074dae7","deepnote_cell_type":"code","id":"7pEnCoNmeFTc"},"source":["class Desired_Pose:\n","    def __init__(self, desired_translation, desired_rotation, desired_wsg = 0.0):\n","        self.desired_translation = desired_translation\n","        self.desired_rotation = desired_rotation\n","        self.desired_wsg = desired_wsg\n","\n","    def __repr__(self):\n","        return f\"Desired_Pose({self.desired_translation}, {self.desired_rotation}, {self.desired_wsg})\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"b3fa913d","is_code_hidden":false,"execution_start":1731976884504,"execution_millis":0,"execution_context_id":"4a5ede74-7bd7-4664-887b-35842c458f57","deepnote_app_is_code_hidden":true,"cell_id":"adfaa31aa2e94de3a6e062b596a67c2d","deepnote_cell_type":"code","id":"HGQDQ10feFTd","executionInfo":{"status":"error","timestamp":1732054151733,"user_tz":300,"elapsed":12,"user":{"displayName":"Amit","userId":"08720702918885979857"}},"outputId":"ebcfd611-473d-48ab-de5c-b0d5f57d66af","colab":{"base_uri":"https://localhost:8080/","height":399}},"source":["from pydrake.all import (\n","    LeafSystem,\n","    AbstractValue,\n","    BasicVector,\n","    DiscreteUpdateEvent,\n","    PiecewisePose\n",")\n","\n","class DynamicTrajectorySource(LeafSystem):\n","    def __init__(self, initial_trajectory):\n","        LeafSystem.__init__(self)\n","        self.DeclareVectorOutputPort(\"trajectory_output\", initial_trajectory.rows(), self.CalcOutput)\n","        self.trajectory = initial_trajectory\n","\n","    def CalcOutput(self, context, output):\n","        time = context.get_time()\n","        # print(self.trajectory.value(time))\n","        output.SetFromVector(self.trajectory.value(time))\n","\n","    def update_trajectory(self, new_trajectory):\n","        self.trajectory = new_trajectory\n","\n","def update_trajectory(trajectory_source: DynamicTrajectorySource, new_trajectory: PiecewisePolynomial):\n","    trajectory_source.update_trajectory(new_trajectory)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'pydrake'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-01b69719252b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pydrake.all import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mLeafSystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mAbstractValue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mBasicVector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mDiscreteUpdateEvent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydrake'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"source_hash":"16ff694b","is_code_hidden":false,"execution_start":1731976884564,"execution_millis":0,"execution_context_id":"4a5ede74-7bd7-4664-887b-35842c458f57","deepnote_app_is_code_hidden":true,"cell_id":"e1ba768d53354727919a82ba14c4d65e","deepnote_cell_type":"code","id":"lWBwt7T0eFTd"},"source":["class SelfMadeArmDriver:\n","    def __init__(\n","        self,\n","        IIWA: IIWA_ARM,\n","        planner: TrajOptPlanner,\n","        dynamic_joint_trajectory_source: DynamicTrajectorySource,\n","        dynamic_wsg_trajectory_source: DynamicTrajectorySource\n","    ):\n","        self.iiwa = IIWA\n","        self.planner = planner\n","        self.dynamic_joint_trajectory_source = dynamic_joint_trajectory_source\n","        self.dynamic_wsg_trajectory_source = dynamic_wsg_trajectory_source\n","\n","    def set_desired_poses_and_follow(self, desired_poses: list[Desired_Pose], start_time = 0.0, dynamic_joint_trajectory_source = None):\n","        pose_positions = [self.iiwa.get_X_WG()]\n","        # print(desired_poses)\n","\n","        # self.iiwa.clear_all_triads()\n","        self.iiwa.visualize_frame(\"current_gripper\", pose_positions[0])\n","        desired_rotation = [0]\n","        desired_wsg_grip = [0.0]\n","        for i, d in enumerate(desired_poses):\n","            # x_w_target = self.iiwa.get_X_WTarget(d.desired_translation)\n","            x_w_target = RigidTransform(RotationMatrix().Identity(), d.desired_translation)\n","            self.iiwa.visualize_frame(f\"gripper_target_{i}\", x_w_target)\n","            pose_positions.append(x_w_target)\n","            desired_rotation.append(d.desired_rotation)\n","            desired_wsg_grip.append(d.desired_wsg)\n","\n","        # print(desired_wsg_grip)\n","        return self.follow_positions(pose_positions, desired_rotation, desired_wsg_grip, start_time)\n","\n","\n","    def generate_grip_values(self, times, initial_wsg_grip, desired_grip, transition_steps = 7):\n","        num_steps = len(times)\n","        grip_values = [initial_wsg_grip] * (num_steps - transition_steps)\n","        transition_values = [initial_wsg_grip + (desired_grip - initial_wsg_grip) * (i / (transition_steps - 1)) for i in range(transition_steps)]\n","        grip_values.extend(transition_values)\n","        return grip_values\n","\n","    def follow_positions_works(\n","        self,\n","        pose_positions: list[RigidTransform],\n","        desired_rotation: list[float],\n","        desired_wsg_grips: list[float],\n","        start_time: float\n","    ):\n","        times, joint_positions, wsg_positions = [], [], []\n","\n","        # print(desired_wsg_grips)\n","\n","        for i, pose in enumerate(pose_positions[:-1]):\n","            start_joint_positions = self.iiwa.find_joint_positions(pose)\n","\n","            start_joint_positions[-1] = desired_rotation[i]\n","\n","            end_joint_positions = self.iiwa.find_joint_positions(pose_positions[i + 1])\n","            end_joint_positions[-1] = desired_rotation[i + 1]\n","\n","            t, q, _ = self.planner.plan(start_joint_positions, end_joint_positions)\n","\n","            wsg_gripper_position = self.generate_grip_values(\n","                t,\n","                desired_wsg_grips[i],\n","                desired_wsg_grips[i+1]\n","            )\n","\n","            times.append(t)\n","            joint_positions.append(q)\n","            wsg_positions.append(wsg_gripper_position)\n","            # print(q[-1])\n","\n","        total_t, total_q, grip_values = self.combine_all_trajectories(times, joint_positions, wsg_positions, start_time)\n","        total_t, total_q= np.array(total_t), np.array(total_q).T\n","        trajectory = PiecewisePolynomial.FirstOrderHold(total_t, total_q)\n","\n","        total_wsg = [np.array([[value]]) for value in grip_values]\n","        wsg_trajectory = PiecewisePolynomial.FirstOrderHold(total_t, total_wsg)\n","\n","        trajectory_source = TrajectorySource(trajectory)\n","        wsg_trajectory_source = TrajectorySource(wsg_trajectory)\n","\n","        self.iiwa.move_arm(trajectory_source, wsg_trajectory_source)\n","\n","        return trajectory.end_time()\n","\n","    def follow_positions(\n","        self,\n","        pose_positions: list[RigidTransform],\n","        desired_rotation: list[float],\n","        desired_wsg_grips: list[float],\n","        start_time: float\n","    ):\n","        times, joint_positions, wsg_positions = [], [], []\n","\n","        # print(desired_wsg_grips)\n","\n","        for i, pose in enumerate(pose_positions[:-1]):\n","            start_joint_positions = self.iiwa.find_joint_positions(pose)\n","\n","            start_joint_positions[-1] = desired_rotation[i]\n","\n","            end_joint_positions = self.iiwa.find_joint_positions(pose_positions[i + 1])\n","            end_joint_positions[-1] = desired_rotation[i + 1]\n","\n","            # print(start_joint_positions, end_joint_positions)\n","            # print()\n","            t, q, _ = self.planner.plan(start_joint_positions, end_joint_positions)\n","\n","            wsg_gripper_position = self.generate_grip_values(\n","                t,\n","                desired_wsg_grips[i],\n","                desired_wsg_grips[i+1]\n","            )\n","\n","            times.append(t)\n","            joint_positions.append(q)\n","            wsg_positions.append(wsg_gripper_position)\n","            # print(q[-1])\n","\n","        total_t, total_q, grip_values = self.combine_all_trajectories(times, joint_positions, wsg_positions, start_time)\n","        # for t, q in zip(total_t, total_q):\n","        #     print(f\"{t}, {q}\")\n","\n","        total_t, total_q= np.array(total_t), np.array(total_q).T\n","        trajectory = PiecewisePolynomial.FirstOrderHold(total_t, total_q)\n","\n","        total_wsg = [np.array([[value]]) for value in grip_values]\n","        wsg_trajectory = PiecewisePolynomial.FirstOrderHold(total_t, total_wsg)\n","\n","        update_trajectory(self.dynamic_joint_trajectory_source, trajectory)\n","        update_trajectory(self.dynamic_wsg_trajectory_source, wsg_trajectory)\n","\n","        return trajectory.end_time()\n","\n","    def combine_all_trajectories(self, times, joint_positions, wsg_positions,start_time: float):\n","        t_res, q_res, wsg_res = times[0] + start_time, joint_positions[0], wsg_positions[0]\n","\n","        for i in range (1, len(times)):\n","            next_time = times[i] + t_res[-1]\n","            t_res = np.concatenate([t_res, next_time[1:]])\n","            q_res = np.concatenate([q_res, joint_positions[i][1:]])\n","            wsg_res = np.concatenate([wsg_res, wsg_positions[i][1:]])\n","\n","        # for t, q in zip(t_res, q_res):\n","        #     print(f\"{t}, {q}\")\n","\n","        return t_res, q_res, wsg_res\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"f2308e7f","execution_start":1731976884624,"execution_millis":0,"execution_context_id":"dc57fb31-3750-44a0-88fd-37509ffb5d7f","cell_id":"766aafd0e7564f6b93e681c01e2b76b5","deepnote_cell_type":"code","id":"HWuB4rHqeFTf"},"source":["class PerceptionModule:\n","    def __init__ (self, diagram, context):\n","        self.cameras = [CameraSystem(i, meshcat, diagram, context) for i in range(3)]\n","        self.diagram = diagram\n","        self.context = context\n","    def update_cameras(self):\n","        for c in self.cameras:\n","            c.compute_camera_data()\n","    def valid_prediction(self, predictions):\n","        return (predictions[0][0][\"boxes\"].size != 0)\n","    def get_camera_data(self):\n","        cameras = self.cameras\n","        rgb_ims = [c.rgb_im for c in cameras]\n","        depth_ims = [c.depth_im for c in cameras]\n","        project_depth_to_pC_funcs = [c.project_depth_to_pC for c in cameras]\n","        X_WCs = [c.X_WC for c in cameras]\n","        return rgb_ims, depth_ims, project_depth_to_pC_funcs, X_WCs\n","    def predict_mask(self, current_time):\n","        cameras = self.cameras\n","        if running_as_notebook and current_time == 1.0:\n","            with torch.no_grad():\n","                predictions = []\n","                predictions.append(\n","                    model([Tf.to_tensor(cameras[0].rgb_im[:, :, :3]).to(device)])\n","                )\n","                predictions.append(\n","                    model([Tf.to_tensor(cameras[1].rgb_im[:, :, :3]).to(device)])\n","                )\n","                predictions.append(\n","                    model([Tf.to_tensor(cameras[2].rgb_im[:, :, :3]).to(device)])\n","                )\n","                for i in range(3):\n","                    for k in predictions[i][0].keys():\n","                        if k == \"masks\":\n","                            predictions[i][0][k] = (\n","                                predictions[i][0][k].mul(255).byte().cpu().numpy()\n","                            )\n","                        else:\n","                            predictions[i][0][k] = predictions[i][0][k].cpu().numpy()\n","            return predictions\n","        else:\n","            return [[{\"boxes\": np.empty((0, 4), dtype=np.float32)}]]\n","    def empty_mask(self):\n","        return [[{\"boxes\": np.empty((0, 4), dtype=np.float32)}]]\n","    def get_antipodal_grasps(self, diagram, context, predictions, X_WG_curr):\n","        label = mode([predictions[0][0]['labels'][0], predictions[1][0]['labels'][0], predictions[2][0]['labels'][0]])[0]\n","\n","        pcd = get_merged_masked_pcd(\n","            predictions, *self.get_camera_data(), label\n","        )\n","\n","        antipodal_grasp = find_antipodal_grasp(self.diagram, self.context, self.cameras, label, predictions)\n","        X_WG = antipodal_grasp\n","        translation = X_WG.translation()\n","        # translation_1 = translation + np.array([0, 0, 0.2])\n","        # translation_2 = translation + np.array([0, 0, 0.4])\n","        rotation = (X_WG_curr.inverse() @ X_WG).rotation().ToRollPitchYaw().yaw_angle()\n","        rotation = abs(rotation)\n","        return  translation, rotation\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"8f4c6c12f27d4a78b3a1e839c5dde05f","deepnote_cell_type":"text-cell-h1","id":"6u1yYSO9eFTg"},"source":["# State Machine"]},{"cell_type":"code","metadata":{"source_hash":"1facf417","is_code_hidden":false,"execution_start":1731976884676,"execution_millis":0,"execution_context_id":"6a8a253c-3265-41b7-8630-d670313ebf77","deepnote_app_is_code_hidden":true,"cell_id":"b64d729e567b4dfab6137131fea20618","deepnote_cell_type":"code","id":"FahobRPveFTg"},"source":["from pydrake.all import (\n","    LeafSystem\n",")\n","\n","from enum import (\n","    Enum\n",")\n","\n","class STATES(int, Enum):\n","    IDLE = 0\n","    FOLLOW = 1\n","\n","\n","class StateMachine(LeafSystem):\n","    def __init__(\n","        self,\n","        builder: DiagramBuilder,\n","        meshcat: MeshcatVisualizer,\n","    ):\n","        LeafSystem.__init__(self)\n","        self.builder = builder\n","        self.meshcat = meshcat\n","        self.DeclarePeriodicDiscreteUpdateEvent(0.1, 0.0, self.Update)\n","\n","        self.iiwa = IIWA_ARM(builder, scenario, meshcat)\n","        self.station = self.iiwa.get_station()\n","        self.plant = self.iiwa.get_plant()\n","        self.planner = TrajOptPlanner()\n","\n","        self.start_time = 0\n","        self.state = STATES.IDLE\n","        self.initial_joint_position = self.iiwa.get_current_gripper_joint_position()\n","        self.predictions = [[{\"boxes\": np.empty((0, 4), dtype=np.float32)}]]\n","\n","        self.initial_gripper_pose = self.iiwa.get_X_WG()\n","        #print(self.initial_gripper_pose)\n","\n","        # Setup initial trajectory\n","        times = np.array([0, 0.1])\n","        positions = np.tile(self.initial_joint_position, (2, 1)).T\n","        self.initial_trajectory = PiecewisePolynomial.FirstOrderHold(times, positions)\n","        self.iiwa_position_input = self.station.GetInputPort(\"iiwa.position\")\n","        self.dynamic_trajectory_source = self.builder.AddSystem(DynamicTrajectorySource(self.initial_trajectory))\n","\n","        # Setup initial wsg trajectory\n","        wsg_positions = np.zeros((1, 2))  # Single joint position, 2 time points\n","        self.initial_wsg_trajectory = PiecewisePolynomial.FirstOrderHold(times, wsg_positions)\n","        self.wsg_input = self.station.GetInputPort(\"wsg.position\")\n","        self.dynamic_wsg_trajectory_source = self.builder.AddSystem(DynamicTrajectorySource(self.initial_wsg_trajectory))\n","\n","        self.ARM = SelfMadeArmDriver(\n","            self.iiwa,\n","            self.planner,\n","            self.dynamic_trajectory_source,\n","            self.dynamic_wsg_trajectory_source\n","        )\n","\n","        self.builder.Connect(self.dynamic_trajectory_source.get_output_port(0), self.iiwa_position_input)\n","        self.builder.Connect(self.dynamic_wsg_trajectory_source.get_output_port(0), self.wsg_input)\n","\n","        self.is_moving = False\n","        self.end_move_time = None\n","        self.cams = None\n","\n","    def set_perception_module(self, diagram, context):\n","        self.perception = PerceptionModule(diagram, context)\n","        self.diagram = diagram\n","        self.context = context\n","        self.predictions = self.perception.empty_mask()\n","    def Update(self, context, event):\n","        diagram = self.diagram\n","        current_time = context.get_time()\n","        #Update the cameras with the current time data\n","        self.perception.update_cameras()\n","\n","        #Run the masking model and get the predictions\n","        predicts = self.perception.predict_mask(current_time)\n","\n","        #If not a valid prediction, don't do anything\n","        if (not self.perception.valid_prediction(predicts) and not self.perception.valid_prediction(self.predictions)):\n","            print(\"Not a valid prediction\")\n","            curr_times_arr = np.array([current_time, current_time + 0.1])\n","\n","            current_position = self.iiwa.get_current_gripper_joint_position()\n","            positions = np.tile(current_position, (2, 1)).T\n","            curr_joint_traj = PiecewisePolynomial.FirstOrderHold(curr_times_arr, positions)\n","\n","            wsg_positions = np.zeros((1, 2))  # Single joint position, 2 time points\n","            curr_wsg_traj = PiecewisePolynomial.FirstOrderHold(curr_times_arr, wsg_positions)\n","\n","\n","            update_trajectory(self.dynamic_trajectory_source, self.initial_trajectory)\n","            update_trajectory(self.dynamic_wsg_trajectory_source, self.initial_wsg_trajectory)\n","        else:\n","            self.predictions = self.predictions if self.perception.valid_prediction(self.predictions) else predicts\n","            print(\"Valid prediction\")\n","\n","            X_WG_curr = state_machine.iiwa.get_X_WG()\n","\n","            #We obtain the antipodal grasps\n","            translation, rotation = self.perception.get_antipodal_grasps(self.diagram, context, self.predictions, X_WG_curr)\n","            translation_1 = translation + np.array([0, 0, 0.2])\n","            translation_2 = translation + np.array([0, 0, 0.4])\n","\n","            follow_poses = [\n","\n","                Desired_Pose(\n","                    translation_1,\n","                    rotation,\n","                    1.0,\n","                ),\n","                Desired_Pose(\n","                    translation,\n","                    rotation,\n","                    1.0,\n","                ),\n","                Desired_Pose(\n","                    translation,\n","                    rotation,\n","                    0.0,\n","                ),\n","                Desired_Pose(\n","                    translation_2,\n","                    rotation,\n","                    0.0,\n","                ),\n","                Desired_Pose(\n","                    np.array([0.15, -0.8, 0.6]),\n","                    rotation,\n","                    0.0,\n","                ),\n","                Desired_Pose(\n","                    np.array([0.15, -0.8, 0.6]),\n","                    rotation,\n","                    1.0,\n","                ),\n","            ]\n","\n","\n","            curr_times_arr = np.array([current_time, current_time + 0.1])\n","\n","            current_position = self.iiwa.get_current_gripper_joint_position()\n","            positions = np.tile(current_position, (2, 1)).T\n","            curr_joint_traj = PiecewisePolynomial.FirstOrderHold(curr_times_arr, positions)\n","\n","            wsg_positions = np.zeros((1, 2))  # Single joint position, 2 time points\n","            curr_wsg_traj = PiecewisePolynomial.FirstOrderHold(curr_times_arr, wsg_positions)\n","\n","\n","            if current_time <= 3.0:\n","                update_trajectory(self.dynamic_trajectory_source, self.initial_trajectory)\n","                update_trajectory(self.dynamic_wsg_trajectory_source, self.initial_wsg_trajectory)\n","            elif not self.is_moving:\n","                self.is_moving = True\n","                self.end_move_time = self.ARM.set_desired_poses_and_follow(follow_poses, current_time)\n","            elif current_time >= self.end_move_time:\n","                self.is_moving = False\n","                update_trajectory(self.dynamic_trajectory_source, curr_joint_traj)\n","                update_trajectory(self.dynamic_wsg_trajectory_source, curr_wsg_traj)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"31279ee5","is_code_hidden":false,"execution_start":1731976884724,"execution_millis":613683,"execution_context_id":"97d0f0bf-b734-4e05-aa2d-e0e6dba16378","deepnote_app_is_code_hidden":true,"cell_id":"30b5121f0a794422a7a358a241f14b7d","deepnote_cell_type":"code","id":"j-jc631BeFTh"},"source":["from pydrake.all import (\n","    ConstantValueSource\n",")\n","\n","scenario = LoadScenario(data=scenario_data)\n","builder = DiagramBuilder()\n","\n","state_machine = StateMachine(builder, meshcat)\n","builder.AddSystem(state_machine)\n","\n","# Export the camera outputs\n","for idx in range(3):  # Assuming three cameras defined in directive\n","    builder.ExportOutput(state_machine.station.GetOutputPort(f\"camera{idx}.rgb_image\"), f\"camera{idx}_rgb_image\")\n","    builder.ExportOutput(state_machine.station.GetOutputPort(f\"camera{idx}.depth_image\"), f\"camera{idx}_depth_image\")\n","\n","to_point_cloud = AddPointClouds(\n","    scenario=scenario, station=state_machine.station, builder=builder, meshcat=meshcat\n",")\n","\n","# Access plant and scene graph to add visual aids (optional)\n","plant = state_machine.station.GetSubsystemByName(\"plant\")\n","scene_graph = state_machine.station.GetSubsystemByName(\"scene_graph\")\n","for idx in range(3):  # Assuming three cameras defined in directive\n","    camera_instance = plant.GetModelInstanceByName(f\"camera{idx}_model\")\n","    AddMultibodyTriad(\n","        plant.GetFrameByName(\"base\", camera_instance),\n","        scene_graph,\n","        length=0.1,\n","        radius=0.005,\n","    )\n","\n","#Export the point cloud output.\n","# for idx in range(3):  # Assuming three cameras defined in directive\n","#     builder.ExportOutput(\n","#         to_point_cloud[f\"camera{idx}\"].point_cloud_output_port(), f\"camera{idx}_point_cloud\"\n","#     )\n","\n","diagram = builder.Build()\n","simulator = Simulator(diagram)\n","context = simulator.get_mutable_context()\n","\n","state_machine.set_perception_module(diagram, context)\n","\n","diagram.ForcedPublish(context)\n","meshcat.StartRecording()\n","\n","simulator.AdvanceTo(20.0)\n","meshcat.PublishRecording()\n","#point_cloud_subsystem_context = to_point_cloud['camera0'].GetMyContextFromRoot(context)\n","#print(to_point_cloud['camera0'].point_cloud_output_port().Eval(point_cloud_subsystem_context))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4929c60d-9325-4a04-bca1-550c19632d0a' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"],"metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","id":"C_OEVl4PeFTi"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"25539fef6ad9486080da127ad4c21f33","colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}}}